<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>cuda的进阶学习</title>
    <url>/2022/03/06/cuda%E7%9A%84%E8%BF%9B%E9%98%B6%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/441146275">[施工中] CUDA GEMM 理论性能分析与 kernel 优化</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/413145211">英伟达GPU架构演进近十年，从费米到安培</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/423992093">CUDA高性能计算经典问题（二）—— 前缀和（Prefix Sum）</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/138668785">处理器和GPU的计算能力如何计算？</a></p>
<p>FFMA理论算力=cuda core <em> 核心频率 </em>2<br>(这里的2表示一个FMA一个时钟周期可以进行2次乘或加的运算)</p>
<p>实际算力=访存比*带宽</p>
<p>关于<a href="https://zhuanlan.zhihu.com/p/441146275">cuda femm</a>中的<a href="https://github.com/Yinghan-Li/YHs_Sample/blob/master/cuda/gemm/sgemm.cu">代码</a>的一些笔记。<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdint&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">random_init</span><span class="params">(<span class="type">float</span> *data, <span class="type">size_t</span> size)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i = <span class="number">0</span>; i &lt; size; ++i) &#123;</span><br><span class="line">        data[i] = <span class="type">float</span>(rand()) / RAND_MAX;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">bool</span> <span class="title function_">check</span><span class="params">(<span class="type">const</span> <span class="type">float</span> *A,</span></span><br><span class="line"><span class="params">           <span class="type">const</span> <span class="type">float</span> *B,</span></span><br><span class="line"><span class="params">           <span class="type">const</span> <span class="type">float</span> *C,</span></span><br><span class="line"><span class="params">           <span class="type">int</span> m, <span class="type">int</span> n, <span class="type">int</span> k)</span> &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; m; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; n; ++j) &#123;</span><br><span class="line">            <span class="type">float</span> sum = <span class="number">0.f</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> p = <span class="number">0</span>; p &lt; k; ++p) &#123;</span><br><span class="line">                sum += A[i * k + p] * B[j + p * n];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (<span class="built_in">std</span>::<span class="built_in">fabs</span>(sum - C[i * n + j]) / <span class="built_in">std</span>::<span class="built_in">fabs</span>(sum) &gt; <span class="number">1e-5</span>f) &#123;</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">&quot;C[%d][%d] not match, %f vs %f\n&quot;</span>, i, j, sum, C[i * n + j]);</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ __forceinline__</span><br><span class="line"><span class="type">uint32_t</span> <span class="title function_">smem_u32addr</span><span class="params">(<span class="type">const</span> <span class="type">void</span> *smem_ptr)</span> &#123;</span><br><span class="line">    <span class="type">uint32_t</span> addr;</span><br><span class="line">    <span class="keyword">asm</span> (<span class="string">&quot;&#123;.reg .u64 u64addr;\n&quot;</span></span><br><span class="line">         <span class="string">&quot; cvta.to.shared.u64 u64addr, %1;\n&quot;</span></span><br><span class="line">         <span class="string">&quot; cvt.u32.u64 %0, u64addr;&#125;\n&quot;</span></span><br><span class="line">         : <span class="string">&quot;=r&quot;</span>(addr)</span><br><span class="line">         : <span class="string">&quot;l&quot;</span>(smem_ptr)</span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> addr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ __forceinline__</span><br><span class="line"><span class="type">void</span> <span class="title function_">ldg32_nc</span><span class="params">(<span class="type">float</span> &amp;reg, <span class="type">const</span> <span class="type">void</span> *ptr, <span class="type">bool</span> guard)</span> &#123;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(</span></span><br><span class="line"><span class="params">        <span class="string">&quot;&#123;.reg .pred p;\n&quot;</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; setp.ne.b32 p, %2, 0;\n&quot;</span></span></span><br><span class="line"><span class="params">#<span class="keyword">if</span> __CUDACC_VER_MAJOR__ &gt;= <span class="number">11</span> &amp;&amp; __CUDACC_VER_MINOR__ &gt;= <span class="number">4</span> &amp;&amp; \</span></span><br><span class="line"><span class="params">    __CUDA_ARCH__ &gt;= <span class="number">750</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; @p ld.global.nc.L2::128B.f32 %0, [%1];&#125;\n&quot;</span></span></span><br><span class="line"><span class="params">#<span class="keyword">else</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; @p ld.global.nc.f32 %0, [%1];&#125;\n&quot;</span></span></span><br><span class="line"><span class="params">#endif</span></span><br><span class="line"><span class="params">        : <span class="string">&quot;=f&quot;</span>(reg)</span></span><br><span class="line"><span class="params">        : <span class="string">&quot;l&quot;</span>(ptr), <span class="string">&quot;r&quot;</span>((<span class="type">int</span>)guard)</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ __forceinline__</span><br><span class="line"><span class="type">void</span> <span class="title function_">ldg32_nc_0</span><span class="params">(<span class="type">float</span> &amp;reg, <span class="type">const</span> <span class="type">void</span> *ptr, <span class="type">bool</span> guard)</span> &#123;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(</span></span><br><span class="line"><span class="params">        <span class="string">&quot;&#123;.reg .pred p;\n&quot;</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; setp.ne.b32 p, %2, 0;\n&quot;</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; @!p mov.b32 %0, 0;\n&quot;</span></span></span><br><span class="line"><span class="params">#<span class="keyword">if</span> __CUDACC_VER_MAJOR__ &gt;= <span class="number">11</span> &amp;&amp; __CUDACC_VER_MINOR__ &gt;= <span class="number">4</span> &amp;&amp; \</span></span><br><span class="line"><span class="params">    __CUDA_ARCH__ &gt;= <span class="number">750</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; @p ld.global.nc.L2::128B.f32 %0, [%1];&#125;\n&quot;</span></span></span><br><span class="line"><span class="params">#<span class="keyword">else</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; @p ld.global.nc.f32 %0, [%1];&#125;\n&quot;</span></span></span><br><span class="line"><span class="params">#endif</span></span><br><span class="line"><span class="params">        : <span class="string">&quot;=f&quot;</span>(reg)</span></span><br><span class="line"><span class="params">        : <span class="string">&quot;l&quot;</span>(ptr), <span class="string">&quot;r&quot;</span>((<span class="type">int</span>)guard)</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ __forceinline__</span><br><span class="line"><span class="type">void</span> <span class="title function_">stg32</span><span class="params">(<span class="type">const</span> <span class="type">float</span> &amp;reg, <span class="type">void</span> *ptr, <span class="type">bool</span> guard)</span> &#123;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(</span></span><br><span class="line"><span class="params">        <span class="string">&quot;&#123;.reg .pred p;\n&quot;</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; setp.ne.b32 p, %2, 0;\n&quot;</span></span></span><br><span class="line"><span class="params">        <span class="string">&quot; @p st.global.f32 [%0], %1;&#125;\n&quot;</span></span></span><br><span class="line"><span class="params">        : : <span class="string">&quot;l&quot;</span>(ptr), <span class="string">&quot;f&quot;</span>(reg), <span class="string">&quot;r&quot;</span>((<span class="type">int</span>)guard)</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ __forceinline__</span><br><span class="line"><span class="type">void</span> <span class="title function_">lds128</span><span class="params">(<span class="type">float</span> &amp;reg0, <span class="type">float</span> &amp;reg1,</span></span><br><span class="line"><span class="params">            <span class="type">float</span> &amp;reg2, <span class="type">float</span> &amp;reg3,</span></span><br><span class="line"><span class="params">            <span class="type">const</span> <span class="type">uint32_t</span> &amp;addr)</span> &#123;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(</span></span><br><span class="line"><span class="params">        <span class="string">&quot;ld.shared.v4.f32 &#123;%0, %1, %2, %3&#125;, [%4];\n&quot;</span></span></span><br><span class="line"><span class="params">        : <span class="string">&quot;=f&quot;</span>(reg0), <span class="string">&quot;=f&quot;</span>(reg1), <span class="string">&quot;=f&quot;</span>(reg2), <span class="string">&quot;=f&quot;</span>(reg3)</span></span><br><span class="line"><span class="params">        : <span class="string">&quot;r&quot;</span>(addr)</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ __forceinline__</span><br><span class="line"><span class="type">void</span> <span class="title function_">sts32</span><span class="params">(<span class="type">const</span> <span class="type">float</span> &amp;reg, <span class="type">const</span> <span class="type">uint32_t</span> &amp;addr)</span> &#123;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(</span></span><br><span class="line"><span class="params">        <span class="string">&quot;st.shared.f32 [%0], %1;\n&quot;</span></span></span><br><span class="line"><span class="params">        : : <span class="string">&quot;r&quot;</span>(addr), <span class="string">&quot;f&quot;</span>(reg)</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__device__ __forceinline__</span><br><span class="line"><span class="type">void</span> <span class="title function_">sts128</span><span class="params">(<span class="type">const</span> <span class="type">float</span> &amp;reg0, <span class="type">const</span> <span class="type">float</span> &amp;reg1,</span></span><br><span class="line"><span class="params">            <span class="type">const</span> <span class="type">float</span> &amp;reg2, <span class="type">const</span> <span class="type">float</span> &amp;reg3,</span></span><br><span class="line"><span class="params">            <span class="type">const</span> <span class="type">uint32_t</span> &amp;addr)</span> &#123;</span><br><span class="line">    <span class="keyword">asm</span> <span class="title function_">volatile</span> <span class="params">(</span></span><br><span class="line"><span class="params">        <span class="string">&quot;st.shared.v4.f32 [%0], &#123;%1, %2, %3, %4&#125;;\n&quot;</span></span></span><br><span class="line"><span class="params">        : : <span class="string">&quot;r&quot;</span>(addr), <span class="string">&quot;f&quot;</span>(reg0), <span class="string">&quot;f&quot;</span>(reg1), <span class="string">&quot;f&quot;</span>(reg2), <span class="string">&quot;f&quot;</span>(reg3)</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">StgFrag</span> &#123;</span></span><br><span class="line">    <span class="type">float</span> data[<span class="number">4</span>][<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">    __device__ __forceinline__</span><br><span class="line">    <span class="title function_">StgFrag</span><span class="params">(<span class="type">const</span> <span class="type">float</span> (&amp;C_frag)[<span class="number">8</span>][<span class="number">8</span>], <span class="type">int</span> tile_x, <span class="type">int</span> tile_y)</span> &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">4</span>; ++j) &#123;</span><br><span class="line">                data[i][j] = C_frag[tile_y * <span class="number">4</span> + i][tile_x * <span class="number">4</span> + j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">__device__ __noinline__</span><br><span class="line"><span class="type">void</span> <span class="title function_">C_tile_wb</span><span class="params">(StgFrag C_frag,</span></span><br><span class="line"><span class="params">               <span class="type">float</span> *C_stg_ptr,</span></span><br><span class="line"><span class="params">               <span class="type">const</span> <span class="type">float</span> *C_lds_ptr,</span></span><br><span class="line"><span class="params">               <span class="type">uint32_t</span> C_sts_addr,</span></span><br><span class="line"><span class="params">               <span class="type">uint32_t</span> m,</span></span><br><span class="line"><span class="params">               <span class="type">uint32_t</span> n,</span></span><br><span class="line"><span class="params">               <span class="type">uint32_t</span> m_idx,</span></span><br><span class="line"><span class="params">               <span class="type">uint32_t</span> n_idx)</span> &#123;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">        sts128(C_frag.data[i][<span class="number">0</span>],</span><br><span class="line">               C_frag.data[i][<span class="number">1</span>],</span><br><span class="line">               C_frag.data[i][<span class="number">2</span>],</span><br><span class="line">               C_frag.data[i][<span class="number">3</span>],</span><br><span class="line">               C_sts_addr + i * <span class="number">8</span> * <span class="keyword">sizeof</span>(float4));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> m_guard = m &lt; m_idx ? <span class="number">0</span> : m - m_idx;</span><br><span class="line"></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">16</span>; ++i) &#123;</span><br><span class="line">        stg32(C_lds_ptr[i * <span class="number">32</span>],</span><br><span class="line">              C_stg_ptr + i * n,</span><br><span class="line">              i &lt; m_guard &amp;&amp; n_idx &lt; n);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * matrix A, B and C: row-major</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * mma block:</span></span><br><span class="line"><span class="comment"> * thread block tile: m128n128k8</span></span><br><span class="line"><span class="comment"> * warp tile: m32n64k8</span></span><br><span class="line"><span class="comment"> * thread tile: m8n8k8</span></span><br><span class="line"><span class="comment"> * thread fragment:</span></span><br><span class="line"><span class="comment"> *     matrixA: 8x1 FP32</span></span><br><span class="line"><span class="comment"> *     matrixB: 1x8 FP32</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * ----------------------------------------------------------------</span></span><br><span class="line"><span class="comment"> * thread block tile map:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *                                128</span></span><br><span class="line"><span class="comment"> *                    --|---------------------|</span></span><br><span class="line"><span class="comment"> *             B_tile  8|                     |</span></span><br><span class="line"><span class="comment"> *                    --|---------------------|</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *  A_tile   | 8 |      |    64    |</span></span><br><span class="line"><span class="comment"> *         --|---|    --|----------|----------|</span></span><br><span class="line"><span class="comment"> *           |   |    32|  warp_0  |  warp_1  |</span></span><br><span class="line"><span class="comment"> *           |   |    --|----------|----------|</span></span><br><span class="line"><span class="comment"> *           |   |      |  warp_2  |  warp_3  |</span></span><br><span class="line"><span class="comment"> *        128|   |      |----------|----------|</span></span><br><span class="line"><span class="comment"> *           |   |      |  warp_4  |  warp_5  |</span></span><br><span class="line"><span class="comment"> *           |   |      |----------|----------|</span></span><br><span class="line"><span class="comment"> *           |   |      |  warp_6  |  warp_7  |</span></span><br><span class="line"><span class="comment"> *         --|---|      |----------|----------|</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * ----------------------------------------------------------------</span></span><br><span class="line"><span class="comment"> * warp tile map:</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * &#x27;z&#x27; thread map to avoid LDS.128 shared memory broadcast limitation.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> *              |              32               ||</span></span><br><span class="line"><span class="comment"> *     B_frag --|---|---|---|---|---|---|---|---||---|---|---|---|---|---|---|---|</span></span><br><span class="line"><span class="comment"> *             1|///|   |   |   |   |   |   |   ||///|   |   |   |   |   |   |   |</span></span><br><span class="line"><span class="comment"> *            --|---|---|---|---|---|---|---|---||---|---|---|---|---|---|---|---|</span></span><br><span class="line"><span class="comment"> * A_frag       | 4 |                           ||</span></span><br><span class="line"><span class="comment"> *    | 1 |                                     ||</span></span><br><span class="line"><span class="comment"> *  --|---|--   |---|---|---|---|---|---|---|---||---|---------------------------|</span></span><br><span class="line"><span class="comment"> *    |///|4    |t0 |t2 |t4 |t6 |t8 |t10|t12|t14||t0 |                           |</span></span><br><span class="line"><span class="comment"> *    |---|--   |---|---|---|---|---|---|---|---||---|                           |</span></span><br><span class="line"><span class="comment"> *    |   |     |t1 |t3 |t5 |t7 |t9 |t11|t13|t15||                               |</span></span><br><span class="line"><span class="comment"> *  16|---|     |---|---|---|---|---|---|---|---||                               |</span></span><br><span class="line"><span class="comment"> *    |   |     |t16|t18|t20|t22|t24|t26|t28|t30||                               |</span></span><br><span class="line"><span class="comment"> *    |---|     |---|---|---|---|---|---|---|---||                               |</span></span><br><span class="line"><span class="comment"> *    |   |     |t17|t19|t21|t23|t25|t27|t29|t31||                               |</span></span><br><span class="line"><span class="comment"> *  ==|===|=====|===|===|===|===|===|===|===|===||===|============================</span></span><br><span class="line"><span class="comment"> *    |///|     |t0 |                           ||t0 |                           |</span></span><br><span class="line"><span class="comment"> *    |---|     |---|                           ||---|                           |</span></span><br><span class="line"><span class="comment"> *    |   |     |                               ||                               |</span></span><br><span class="line"><span class="comment"> *    |---|     |                               ||                               |</span></span><br><span class="line"><span class="comment"> *    |   |     |                               ||                               |</span></span><br><span class="line"><span class="comment"> *    |---|     |                               ||                               |</span></span><br><span class="line"><span class="comment"> *    |   |     |                               ||                               |</span></span><br><span class="line"><span class="comment"> *    |---|     |-------------------------------||-------------------------------|</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">__global__ __launch_bounds__(<span class="number">256</span>, <span class="number">2</span>)<span class="comment">//__launch_bounds__(maxThreadsPerBlock, minBlocksPerMultiprocessor)</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sgemm_128x128x8_kernel</span><span class="params">(<span class="type">const</span> <span class="type">float</span> *A,</span></span><br><span class="line"><span class="params">                            <span class="type">const</span> <span class="type">float</span> *B,</span></span><br><span class="line"><span class="params">                            <span class="type">float</span> *C,</span></span><br><span class="line"><span class="params">                            <span class="type">uint32_t</span> m,</span></span><br><span class="line"><span class="params">                            <span class="type">uint32_t</span> n,</span></span><br><span class="line"><span class="params">                            <span class="type">uint32_t</span> k,</span></span><br><span class="line"><span class="params">                            <span class="type">uint32_t</span> A_ldg_step,    <span class="comment">// k * sizeof(float)</span></span></span><br><span class="line"><span class="params">                            <span class="type">uint32_t</span> B_ldg_step)</span> &#123;  <span class="comment">// n * sizeof(float) * 8</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * matrix A &amp; B thread block tile shared memory (double buffer)</span></span><br><span class="line"><span class="comment">     * matrix A: 132 * 8 * 4Byte/item * double buffer = 4.125KB * 2</span></span><br><span class="line"><span class="comment">     * matrix B: 128 * 8 * 4Byte/item * double buffer = 8KB</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * for double buffer faster switch, A_smem requires 8KB * 2 shared memory</span></span><br><span class="line"><span class="comment">     * and 16KB aligned, B_smem should be 8KB aligned, then the double buffer</span></span><br><span class="line"><span class="comment">     * can be switched by only 1 xor instruction:</span></span><br><span class="line"><span class="comment">     *     (uint32_t &amp;)A_smem ^= 0x2000;</span></span><br><span class="line"><span class="comment">     *     (uint32_t &amp;)B_smem ^= 0x1000;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    __shared__ __align__(<span class="number">16</span> * <span class="number">1024</span>) <span class="type">char</span> smem[<span class="number">24</span> * <span class="number">1024</span>];</span><br><span class="line">    <span class="type">float</span> *A_smem = reinterpret_cast&lt;<span class="type">float</span> *&gt;(smem);</span><br><span class="line">    <span class="type">float</span> *B_smem = reinterpret_cast&lt;<span class="type">float</span> *&gt;(smem + <span class="number">16</span> * <span class="number">1024</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// A, B and C register fragment</span></span><br><span class="line">    <span class="type">float</span> A_frag[<span class="number">2</span>][<span class="number">8</span>];</span><br><span class="line">    <span class="type">float</span> B_frag[<span class="number">2</span>][<span class="number">8</span>];</span><br><span class="line">    <span class="type">float</span> C_frag[<span class="number">8</span>][<span class="number">8</span>];</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; ++i) &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">8</span>; ++j) &#123;</span><br><span class="line">            C_frag[i][j] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> lane_id = threadIdx.x % <span class="number">32</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> warp_id = threadIdx.x / <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4x8 threads each warp for FFMA</span></span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> mma_tid_x = (lane_id / <span class="number">2</span>) % <span class="number">8</span>;</span><br><span class="line">    <span class="type">const</span> <span class="type">uint32_t</span> mma_tid_y = (lane_id / <span class="number">16</span>) * <span class="number">2</span> + (lane_id % <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// A_tile &amp; B_tile ldg pointer</span></span><br><span class="line">    <span class="type">const</span> <span class="type">char</span> *A_ldg_ptr = (<span class="type">const</span> <span class="type">char</span> *)(</span><br><span class="line">        A + (blockIdx.y * <span class="number">128</span> + threadIdx.x / <span class="number">8</span> * <span class="number">4</span>) * k + threadIdx.x % <span class="number">8</span>);</span><br><span class="line">    <span class="type">const</span> <span class="type">char</span> *B_ldg_ptr = (<span class="type">const</span> <span class="type">char</span> *)(</span><br><span class="line">        B + (threadIdx.x / <span class="number">32</span>) * n + blockIdx.x * <span class="number">128</span> + threadIdx.x % <span class="number">32</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// A_tile &amp; B_tile sts/lds pointer</span></span><br><span class="line">    <span class="comment">// using uint32_t pointer for faster double buffer switch</span></span><br><span class="line">    <span class="type">uint32_t</span> A_sts_addr = smem_u32addr(</span><br><span class="line">        A_smem + (threadIdx.x % <span class="number">8</span>) * <span class="number">132</span> + (threadIdx.x / <span class="number">8</span>) * <span class="number">4</span>);</span><br><span class="line">    <span class="type">uint32_t</span> B_sts_addr = smem_u32addr(</span><br><span class="line">        B_smem + (threadIdx.x / <span class="number">32</span>) * <span class="number">128</span> + (threadIdx.x % <span class="number">32</span>));</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> A_lds_addr = smem_u32addr(</span><br><span class="line">        A_smem + (warp_id / <span class="number">2</span>) * <span class="number">32</span> + mma_tid_y * <span class="number">4</span>);</span><br><span class="line">    <span class="type">uint32_t</span> B_lds_addr = smem_u32addr(</span><br><span class="line">        B_smem + (warp_id % <span class="number">2</span>) * <span class="number">64</span> + mma_tid_x * <span class="number">4</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// ldg_guard to avoid LDG out of bound</span></span><br><span class="line">    <span class="type">uint32_t</span> A_ldg_guard = <span class="number">0</span>;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">        <span class="type">int</span> m_idx = blockIdx.y * <span class="number">128</span> + threadIdx.x / <span class="number">8</span> * <span class="number">4</span> + i;</span><br><span class="line">        <span class="keyword">if</span> (m_idx &lt; m) &#123;</span><br><span class="line">            A_ldg_guard |= (<span class="number">1u</span> &lt;&lt; i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> B_ldg_guard = <span class="number">0</span>;</span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">        <span class="type">int</span> n_idx = blockIdx.x * <span class="number">128</span> + threadIdx.x % <span class="number">32</span> + i * <span class="number">32</span>;</span><br><span class="line">        <span class="keyword">if</span> (n_idx &lt; n) &#123;</span><br><span class="line">            B_ldg_guard |= (<span class="number">1u</span> &lt;&lt; i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> A_ldg_reg[<span class="number">4</span>];</span><br><span class="line">    <span class="type">float</span> B_ldg_reg[<span class="number">4</span>];</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1&#x27;st A&amp;B tile loaded before the k_tile loop</span></span><br><span class="line">    <span class="type">uint32_t</span> k_tiles = (k + <span class="number">7</span>) / <span class="number">8</span> - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// load 1&#x27;st tile to shared memory</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">uint32_t</span> first_k_tile = k - k_tiles * <span class="number">8</span>;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">            <span class="type">bool</span> guard = (A_ldg_guard &amp; (<span class="number">1u</span> &lt;&lt; i)) != <span class="number">0</span> &amp;&amp;</span><br><span class="line">                         threadIdx.x % <span class="number">8</span> &lt; first_k_tile;</span><br><span class="line">            ldg32_nc_0(A_ldg_reg[i],</span><br><span class="line">                       A_ldg_ptr + i * A_ldg_step,</span><br><span class="line">                       guard);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        sts128(A_ldg_reg[<span class="number">0</span>], A_ldg_reg[<span class="number">1</span>], A_ldg_reg[<span class="number">2</span>], A_ldg_reg[<span class="number">3</span>],</span><br><span class="line">               A_sts_addr);</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">            <span class="type">bool</span> guard = (B_ldg_guard &amp; (<span class="number">1u</span> &lt;&lt; i)) != <span class="number">0</span> &amp;&amp;</span><br><span class="line">                         threadIdx.x / <span class="number">32</span> &lt; first_k_tile;</span><br><span class="line">            ldg32_nc_0(B_ldg_reg[i],</span><br><span class="line">                       B_ldg_ptr + i * <span class="number">32</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>),</span><br><span class="line">                       guard);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">            sts32(B_ldg_reg[i], B_sts_addr + i * <span class="number">32</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// switch double buffer</span></span><br><span class="line">        A_sts_addr ^= <span class="number">0x2000</span>;</span><br><span class="line">        B_sts_addr ^= <span class="number">0x1000</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// ldg pointer for next tile</span></span><br><span class="line">        A_ldg_ptr += first_k_tile * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">        B_ldg_ptr += n * first_k_tile * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// load 1&#x27;st fragment</span></span><br><span class="line">    lds128(A_frag[<span class="number">0</span>][<span class="number">0</span>], A_frag[<span class="number">0</span>][<span class="number">1</span>], A_frag[<span class="number">0</span>][<span class="number">2</span>], A_frag[<span class="number">0</span>][<span class="number">3</span>],</span><br><span class="line">           A_lds_addr);</span><br><span class="line">    lds128(A_frag[<span class="number">0</span>][<span class="number">4</span>], A_frag[<span class="number">0</span>][<span class="number">5</span>], A_frag[<span class="number">0</span>][<span class="number">6</span>], A_frag[<span class="number">0</span>][<span class="number">7</span>],</span><br><span class="line">           A_lds_addr + <span class="number">16</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    lds128(B_frag[<span class="number">0</span>][<span class="number">0</span>], B_frag[<span class="number">0</span>][<span class="number">1</span>], B_frag[<span class="number">0</span>][<span class="number">2</span>], B_frag[<span class="number">0</span>][<span class="number">3</span>],</span><br><span class="line">           B_lds_addr);</span><br><span class="line">    lds128(B_frag[<span class="number">0</span>][<span class="number">4</span>], B_frag[<span class="number">0</span>][<span class="number">5</span>], B_frag[<span class="number">0</span>][<span class="number">6</span>], B_frag[<span class="number">0</span>][<span class="number">7</span>],</span><br><span class="line">           B_lds_addr + <span class="number">32</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// k_tiles loop</span></span><br><span class="line">    <span class="keyword">for</span> (; k_tiles &gt; <span class="number">0</span>; --k_tiles) &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> k_frag = <span class="number">0</span>; k_frag &lt; <span class="number">8</span>; ++k_frag) &#123;</span><br><span class="line">            <span class="comment">// store next A&amp;B tile to shared memory</span></span><br><span class="line">            <span class="keyword">if</span> (k_frag == <span class="number">7</span>) &#123;</span><br><span class="line">                sts128(A_ldg_reg[<span class="number">0</span>], A_ldg_reg[<span class="number">1</span>], A_ldg_reg[<span class="number">2</span>], A_ldg_reg[<span class="number">3</span>],</span><br><span class="line">                       A_sts_addr);</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">                    sts32(B_ldg_reg[i], B_sts_addr + i * <span class="number">32</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                __syncthreads();</span><br><span class="line"></span><br><span class="line">                <span class="comment">// switch double buffer</span></span><br><span class="line">                A_lds_addr ^= <span class="number">0x2000</span>;</span><br><span class="line">                B_lds_addr ^= <span class="number">0x1000</span>;</span><br><span class="line">                A_sts_addr ^= <span class="number">0x2000</span>;</span><br><span class="line">                B_sts_addr ^= <span class="number">0x1000</span>;</span><br><span class="line"></span><br><span class="line">                <span class="comment">// ldg pointer for next tile</span></span><br><span class="line">                A_ldg_ptr += <span class="number">8</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">                B_ldg_ptr += B_ldg_step;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// load next A&amp;B fragment from shared memory to register</span></span><br><span class="line">            lds128(A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">0</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">1</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">2</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">3</span>],</span><br><span class="line">                   A_lds_addr + (k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">132</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">            lds128(A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">4</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">5</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">6</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">7</span>],</span><br><span class="line">                   A_lds_addr + ((k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">132</span> + <span class="number">16</span>) * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">            lds128(B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">0</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">1</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">2</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">3</span>],</span><br><span class="line">                   B_lds_addr + (k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">128</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">            lds128(B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">4</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">5</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">6</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">7</span>],</span><br><span class="line">                   B_lds_addr + ((k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">128</span> + <span class="number">32</span>) * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// load next A&amp;B tile</span></span><br><span class="line">            <span class="keyword">if</span> (k_frag == <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">                    ldg32_nc(A_ldg_reg[i],</span><br><span class="line">                             A_ldg_ptr + i * A_ldg_step,</span><br><span class="line">                             (A_ldg_guard &amp; (<span class="number">1u</span> &lt;&lt; i)) != <span class="number">0</span>);</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">4</span>; ++i) &#123;</span><br><span class="line">                    ldg32_nc(B_ldg_reg[i],</span><br><span class="line">                             B_ldg_ptr + i * <span class="number">32</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>),</span><br><span class="line">                             (B_ldg_guard &amp; (<span class="number">1u</span> &lt;&lt; i)) != <span class="number">0</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// FFMA loop</span></span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; ++i) &#123;</span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">8</span>; ++j) &#123;</span><br><span class="line">                    C_frag[i][j] += A_frag[k_frag % <span class="number">2</span>][i] *</span><br><span class="line">                                    B_frag[k_frag % <span class="number">2</span>][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// FFMA for the last tile</span></span><br><span class="line">    <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> k_frag = <span class="number">0</span>; k_frag &lt; <span class="number">8</span>; ++k_frag) &#123;</span><br><span class="line">        <span class="keyword">if</span> (k_frag &lt; <span class="number">7</span>) &#123;</span><br><span class="line">            <span class="comment">// load next A&amp;B fragment from shared memory to register</span></span><br><span class="line">            lds128(A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">0</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">1</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">2</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">3</span>],</span><br><span class="line">                   A_lds_addr + (k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">132</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">            lds128(A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">4</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">5</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">6</span>],</span><br><span class="line">                   A_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">7</span>],</span><br><span class="line">                   A_lds_addr + ((k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">132</span> + <span class="number">16</span>) * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">            lds128(B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">0</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">1</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">2</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">3</span>],</span><br><span class="line">                   B_lds_addr + (k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">128</span> * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">            lds128(B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">4</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">5</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">6</span>],</span><br><span class="line">                   B_frag[(k_frag + <span class="number">1</span>) % <span class="number">2</span>][<span class="number">7</span>],</span><br><span class="line">                   B_lds_addr + ((k_frag + <span class="number">1</span>) % <span class="number">8</span> * <span class="number">128</span> + <span class="number">32</span>) * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// FFMA loop</span></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">8</span>; ++i) &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">8</span>; ++j) &#123;</span><br><span class="line">                C_frag[i][j] += A_frag[k_frag % <span class="number">2</span>][i] *</span><br><span class="line">                                B_frag[k_frag % <span class="number">2</span>][j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// C_tile write back, reuse A&amp;B tile shared memory buffer</span></span><br><span class="line">    <span class="type">uint32_t</span> C_sts_addr = smem_u32addr((float4 *)(smem + warp_id * <span class="number">2048</span>) +</span><br><span class="line">                                       mma_tid_y * <span class="number">4</span> * <span class="number">8</span> + mma_tid_x);</span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> *C_lds_ptr = (<span class="type">float</span> *)(smem + warp_id * <span class="number">2048</span>) + lane_id;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> m_idx = blockIdx.y * <span class="number">128</span> + warp_id / <span class="number">2</span> * <span class="number">32</span>;</span><br><span class="line">    <span class="type">uint32_t</span> n_idx = blockIdx.x * <span class="number">128</span> + warp_id % <span class="number">2</span> * <span class="number">64</span> + lane_id;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *C_stg_ptr = C + m_idx * n + n_idx;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (m_idx &gt;= m) &#123;</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (m_idx + <span class="number">32</span> &lt;= m) &#123;</span><br><span class="line">        <span class="type">uint32_t</span> n_guard = n &lt; n_idx ? <span class="number">0</span> : n - n_idx;</span><br><span class="line"></span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">                __syncthreads();</span><br><span class="line"></span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> p = <span class="number">0</span>; p &lt; <span class="number">4</span>; ++p) &#123;</span><br><span class="line">                    sts128(C_frag[i * <span class="number">4</span> + p][j * <span class="number">4</span>],</span><br><span class="line">                           C_frag[i * <span class="number">4</span> + p][j * <span class="number">4</span> + <span class="number">1</span>],</span><br><span class="line">                           C_frag[i * <span class="number">4</span> + p][j * <span class="number">4</span> + <span class="number">2</span>],</span><br><span class="line">                           C_frag[i * <span class="number">4</span> + p][j * <span class="number">4</span> + <span class="number">3</span>],</span><br><span class="line">                           C_sts_addr + p * <span class="number">8</span> * <span class="keyword">sizeof</span>(float4));</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                __syncthreads();</span><br><span class="line"></span><br><span class="line">                <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">                <span class="keyword">for</span> (<span class="type">int</span> p = <span class="number">0</span>; p &lt; <span class="number">16</span>; ++p) &#123;</span><br><span class="line">                    stg32(C_lds_ptr[p * <span class="number">32</span>],</span><br><span class="line">                          C_stg_ptr + (i * <span class="number">16</span> + p) * n + j * <span class="number">32</span>,</span><br><span class="line">                          j * <span class="number">32</span> &lt; n_guard);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">2</span>; ++i) &#123;</span><br><span class="line">            <span class="meta">#<span class="keyword">pragma</span> unroll</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> j = <span class="number">0</span>; j &lt; <span class="number">2</span>; ++j) &#123;</span><br><span class="line">                StgFrag stg_frag(C_frag, j, i);</span><br><span class="line"></span><br><span class="line">                C_tile_wb(stg_frag,</span><br><span class="line">                          C_stg_ptr + i * <span class="number">16</span> * n + j * <span class="number">32</span>,</span><br><span class="line">                          C_lds_ptr,</span><br><span class="line">                          C_sts_addr,</span><br><span class="line">                          m,</span><br><span class="line">                          n,</span><br><span class="line">                          m_idx + i * <span class="number">16</span>,</span><br><span class="line">                          n_idx + j * <span class="number">32</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    <span class="type">int</span> m = <span class="number">5120</span>;</span><br><span class="line">    <span class="type">int</span> n = <span class="number">4096</span>;</span><br><span class="line">    <span class="type">int</span> k = <span class="number">4096</span>;</span><br><span class="line">    <span class="type">int</span> n_iter = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *h_A, *h_B, *h_C;</span><br><span class="line">    cudaMallocHost(&amp;h_A, m * k * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    cudaMallocHost(&amp;h_B, k * n * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    cudaMallocHost(&amp;h_C, m * n * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    random_init(h_A, m * k);</span><br><span class="line">    random_init(h_B, k * n);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> *d_A, *d_B, *d_C;</span><br><span class="line">    cudaMalloc(&amp;d_A, m * k * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    cudaMalloc(&amp;d_B, k * n * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">    cudaMalloc(&amp;d_C, m * n * <span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line"></span><br><span class="line">    cudaMemcpy(d_A, h_A, m * k * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyDefault);</span><br><span class="line">    cudaMemcpy(d_B, h_B, k * n * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyDefault);</span><br><span class="line"></span><br><span class="line">    cudaEvent_t start, end;</span><br><span class="line">    cudaEventCreate(&amp;start);</span><br><span class="line">    cudaEventCreate(&amp;end);</span><br><span class="line"></span><br><span class="line">    dim3 <span class="title function_">grid</span><span class="params">((n + <span class="number">127</span>) / <span class="number">128</span>, (m + <span class="number">127</span>) / <span class="number">128</span>)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// warmup</span></span><br><span class="line">    sgemm_128x128x8_kernel&lt;&lt;&lt;grid, <span class="number">256</span>&gt;&gt;&gt;(</span><br><span class="line">        d_A, d_B, d_C, m, n, k, k * <span class="keyword">sizeof</span>(<span class="type">float</span>), n * <span class="keyword">sizeof</span>(<span class="type">float</span>) * <span class="number">8</span>);</span><br><span class="line"></span><br><span class="line">    cudaEventRecord(start);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n_iter; ++i) &#123;</span><br><span class="line">        sgemm_128x128x8_kernel&lt;&lt;&lt;grid, <span class="number">256</span>&gt;&gt;&gt;(</span><br><span class="line">            d_A, d_B, d_C, m, n, k, k * <span class="keyword">sizeof</span>(<span class="type">float</span>), n * <span class="keyword">sizeof</span>(<span class="type">float</span>) * <span class="number">8</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    cudaEventRecord(end);</span><br><span class="line">    cudaEventSynchronize(end);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> ms;</span><br><span class="line">    cudaEventElapsedTime(&amp;ms, start, end);</span><br><span class="line">    cudaEventDestroy(start);</span><br><span class="line">    cudaEventDestroy(end);</span><br><span class="line"></span><br><span class="line">    <span class="type">long</span> workload = n_iter * <span class="type">long</span>(m) * n * k * <span class="number">2</span>;</span><br><span class="line">    <span class="type">double</span> gflops = (<span class="type">double</span>(workload) / <span class="number">1e9</span>) / (<span class="type">double</span>(ms) / <span class="number">1e3</span>);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Performance: %fGFLOPS\n&quot;</span>, gflops);</span><br><span class="line"></span><br><span class="line">    cudaMemcpy(h_C, d_C, m * n * <span class="keyword">sizeof</span>(<span class="type">float</span>), cudaMemcpyDefault);</span><br><span class="line"></span><br><span class="line">    <span class="type">bool</span> chk = check(h_A, h_B, h_C, m, n, k);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Matrix_C check: %s\n&quot;</span>, chk ? <span class="string">&quot;OK&quot;</span> : <span class="string">&quot;Failed&quot;</span>);</span><br><span class="line"></span><br><span class="line">    cudaFree(d_A);</span><br><span class="line">    cudaFree(d_B);</span><br><span class="line">    cudaFree(d_C);</span><br><span class="line">    cudaFreeHost(h_A);</span><br><span class="line">    cudaFreeHost(h_B);</span><br><span class="line">    cudaFreeHost(h_C);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
]]></content>
      <categories>
        <category>cuda基础学习</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title>online CP decomposition</title>
    <url>/2022/05/15/online-CP-decomposition/</url>
    <content><![CDATA[<h1 id="类似知识"><a href="#类似知识" class="headerlink" title="类似知识"></a>类似知识</h1><h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>待写</p>
<h2 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h2><p>待写</p>
<h1 id="Tensor-Decomposition"><a href="#Tensor-Decomposition" class="headerlink" title="Tensor Decomposition"></a>Tensor Decomposition</h1><p>用于feature extraction、dimensionality reduction和knowledge discovery。随着时间的变化，张量的数据可能增加、减少或者修改在他任何的维度上。现实生活中最常用的动态张量就是随着时间变化的数据，而其它的维度保持不变。这种张量称为 online tensors, tensor streams incremental tensors.<br>在大规模的online tensor中，找到其分解结果是很困难的，他的难点主要是</p>
<ul>
<li>online tensors are growing with time,他的整体大小其实是没有限制的。TD 需要高效和可扩展，从时间和空间来看。</li>
<li>高数据生成速率需要分解方法提供实时或接近实时的性能。</li>
</ul>
<h1 id="Notation-and-basic-operations"><a href="#Notation-and-basic-operations" class="headerlink" title="Notation and basic operations"></a>Notation and basic operations</h1><div class="table-container">
<table>
<thead>
<tr>
<th>symbol</th>
<th>definition</th>
</tr>
</thead>
<tbody>
<tr>
<td>$A^{T}$</td>
<td>转置 transpose</td>
</tr>
<tr>
<td>$A^{-1}$</td>
<td>逆 inverse</td>
</tr>
<tr>
<td>$A^{\dagger}$</td>
<td>Moore-Penrose pseudoinverse</td>
</tr>
<tr>
<td>$\lVert A\rVert$</td>
<td>frobenius norm</td>
</tr>
<tr>
<td>$\odot$</td>
<td>khatri-rao product</td>
</tr>
<tr>
<td>$\otimes$</td>
<td>hadamard product</td>
</tr>
<tr>
<td>$X_{(n)}$</td>
<td>张量的mode-n展开</td>
</tr>
</tbody>
</table>
</div>
<h1 id="预知识"><a href="#预知识" class="headerlink" title="预知识"></a>预知识</h1><h2 id="CP分解"><a href="#CP分解" class="headerlink" title="CP分解"></a>CP分解</h2><p>cp分解被广泛用于探索多维数据的潜在结构。给定一个N阶的张量$\mathcal{X}\in \mathbb{R}^{I_1\times \cdots \times I_N}$，CP分解近似这个张量通过N个loading 矩阵，因此</p>
<script type="math/tex; mode=display">
\begin{equation}
X_{(n)}\approx A^{(n)}(A^{(N)}\odot \cdots A^{(n+1)}\odot A^{(n-1)}\cdots A^{(1)})^T  =A^{(n)}(\odot^N_{i\neq n}A^{(i)})^T=[A^{(1)},\cdots,A^{(N)}] 
\end{equation}</script><p>其中$[\cdot]$定义为cp分解符号。<br>为了找到CP分解的结果，目标函数是最小化估计误差 $\mathcal{L}$，定义为</p>
<script type="math/tex; mode=display">
\mathcal{L}=\frac{1}{2}\| X_{(n)}-A^{(n)}(\odot^N_{i\neq n}A^{(i)})^T\|^2</script><p>但是直接最小化$\mathcal{L}$很困难，因为$\mathcal{L}$关于$A^{(1)},\cdots,A^{(N)}$是nonconvex的，因此常用的办法是使用ALS，通过固定n-th矩阵，交替最小化，此时$\mathcal{X}$关于$A^{(n)}$是convex的，此时</p>
<script type="math/tex; mode=display">
A^{(n)}\gets argmin_{A^{(n)}} \frac{1}{2}\| X_{(n)}-A^{(n)}(\odot^N_{i\neq n}A^{(i)})^T\|^2</script><h2 id="online-CP分解"><a href="#online-CP分解" class="headerlink" title="online CP分解"></a>online CP分解</h2><p>给出一个三维的online tensor$\mathcal{X}\in \mathbb{X}^{I\times J\times (t_{old}+t_{new})}$，这个$\mathcal{X}$是通过对$\mathcal{X}_{old}\in \mathbb{R}^{I\times J\times t_{old}}$的最后一个mode进行追加$\mathcal{X}_{new}\in \mathbb{R}^{I\times J\times t_{new}}$.通常追加的张量很小，因此假设$\mathcal{X}_{new}\ll \mathcal{X}_{old}$. $\mathcal{X}_{old}$的cp分解写作$[A_{old},B_{old},C_{old}]$.目标是找到$\mathcal{X}$的cp分解$[A,B,C]$.<br>针对三阶张量</p>
<script type="math/tex; mode=display">
C\gets argmin_{C} \frac{1}{2}\| X_{(3)}-C(B\odot A)^T\|^2</script><blockquote id="fn__^">
<sup>_^</sup>. 下面介绍一个现有的online cp分解的方法<a href="#reffn__^" title="Jump back to footnote [_^] in the text."> &#8617;</a>
</blockquote>
<p><strong>SDT and RLST</strong>  这两种方法都是将online张量分解问题转换成增量矩阵分解问题，令$D=A\odot B$, 因此公式(1)可以被写成$X_{(3)}=CD^T$，问题就转变为如何去估计C和D。这两种方法不同之处在于计算C和D。SDT选择SVD来进行$ X_{old(3)}=U_{old}\sum_{old}V^T_{old}$，这时总有一个矩阵$W_{old}$使得$C_{old}=U_{old}W^{-1}_{old}$, $D_{old}=V_{old}\sum_{old}W_{old}^T$.同理得到，可以找到一个矩阵$W$，使得$C=UW^{-1}$, $D=V\sum W^T$,其中$U\sum V^T$是$X_{(3)}$的SVD分解。作者假设$D$和$D_{old}$只有微小的区别，因此</p>
<p>接下来介绍Zhou^[Zhou S, Vinh N X, Bailey J, et al. Accelerating Online CP Decompositions for Higher Order Tensors[A]. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining[C]. New York, NY, USA: Association for Computing Machinery, 2016: 1375–1384.]的做法。</p>
<h2 id="update-temporal-mode-C"><a href="#update-temporal-mode-C" class="headerlink" title="update temporal mode C"></a>update temporal mode C</h2><p>首先固定矩阵$A,B$,然后更新$C$，以此类推更新AB。</p>
<script type="math/tex; mode=display">
\begin{aligned}
C &\gets argmin_{C} \frac{1}{2}\| X_{(3)}-C(B\odot A)^T\|^2 \\
&=argmin_{C} \frac{1}{2}\left \lVert \left [ \begin{aligned}X_{old(3)}\\X_{new(3)}\\\end{aligned}\right]- \left [ \begin{aligned}C^{(1)}\\C^{(2)}\\\end{aligned}\right](B\odot A)^T\right\rVert^2\\
&=argmin_{C} \frac{1}{2}\left \lVert \left [ \begin{aligned}X_{old(3)}-C^{(1)}(B\odot A)^T    \\X_{new(3)}-C^{(2)}(B\odot A)^T \\\end{aligned}\right]\right\rVert^2
\end{aligned}</script><p>第一行通过固定$A_{old} B_{old}$得到$C_{old}$来最小化范数，即</p>
<script type="math/tex; mode=display">
C_{old}(C^{(1)})=X_{old(3)}((B_{old}\odot A_{old})^T)^\dagger</script><p>在关于t的online张量中，$C_{old}$是已经计算好的，$A$和$B$用$A_{old} B_{old}$。因此上面的最小化问题其实只要最小化第二行，整个问题变成计算</p>
<script type="math/tex; mode=display">
C_{new}(C^{(2)})=X_{new(3)}((B\odot A)^T)^\dagger</script><p>综合上述得到</p>
<script type="math/tex; mode=display">
C=
\left [\begin{aligned}C_{old}  \\C_{new} \\\end{aligned}\right ]
=\left [\begin{aligned}C_{old}&  \\X_{new(3)}((B\odot &A)^T)^\dagger\\\end{aligned}
\right ]</script><h2 id="update-non-temporal-modes-A-and-B"><a href="#update-non-temporal-modes-A-and-B" class="headerlink" title="update non-temporal modes A and B"></a>update non-temporal modes A and B</h2><p>首先更新$A$。通过固定$B$和$C$，这个误差估计函数$\mathcal{L}$可以写成$\frac{1}{2}| X_{(1)}-A(C\odot B)^T|^2$，其导数为</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial A}=X_{(1)}(C\odot B)-A(C\odot B)^T(C\odot B)</script><p>设置导数为0，并令$P=X_{(1)}(C\odot B)$以及$Q=(C\odot B)^T(C\odot B)$可以得到</p>
<script type="math/tex; mode=display">A=PQ^{-1}</script><p>直接去计算$P$和$Q$是很费时的，主要是因为$(C\odot B)$是一个huge矩阵。<br>为了提升效率，需要一种更快的方法。</p>
<script type="math/tex; mode=display">
\begin{aligned}
P&=X_{(1)}(C\odot B)\\
 &=[X_{old(1)},X_{new(1)}]\left(\left[\begin{aligned}C_{old}  \\C_{new} \\\end{aligned}\right] \odot B  \right)\\
 &=[X_{old(1)},X_{new(1)}]\left[\begin{aligned}C_{old}\odot B   \\C_{new}\odot B  \\\end{aligned}\right]\\
 &=X_{old(1)}(C_{old}\odot B )+X_{new(1)}(C_{new}\odot B)
\end{aligned}</script><p>注意到$B$可以固定为$B_{old}$,因此上式中的最后一行的第一部分其实是上次迭代的结果，因此上式可以写成</p>
<script type="math/tex; mode=display">
P=P_{old}+X_{new(1)}(C_{new}\odot B)</script><p>这意味着通过保存之前的$P$可以避免大量的计算，将P初始化为$\mathcal{X}$的一部分。这样无论何时新的数据进来，$P$都可以高效计算。<br>同样地，$Q$可以估计为</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q&=Q_{old}+(C_{new}\odot B)^T(C_{new}\odot B)\\
&=Q_{old}+(C^T_{new}C_{new})\otimes (B^TB)
\end{aligned}</script><p>结合上一次的ALS迭代结果，矩阵$A$的更新规则为</p>
<script type="math/tex; mode=display">
\begin{aligned}
P&\gets P+X_{new(1)}(C_{new}\odot B)\\
Q&\gets Q+(C^T_{new}C_{new})\otimes (B^TB)\\
A&\gets PQ^{-1}\\
\end{aligned}</script><p>同样地$B$的更新规则为</p>
<script type="math/tex; mode=display">
\begin{aligned}
U&\gets U+X_{new(2)}(C_{new}\odot A)\\
V&\gets V+(C^T_{new}C_{new})\otimes (A^TA)\\
B&\gets UV^{-1}
\end{aligned}</script><p><strong>总结</strong>对于随着时间增长的三阶张量，提出了称为OnlineCP的算法，分为以下两步</p>
<ol>
<li><strong>初始化</strong>：对于non-temporal 模态，矩阵$PQUV$使用$\mathcal{X}_{init}$和$[![A,B,C]!]$进行初始化，即<script type="math/tex; mode=display">
\begin{aligned}
P=X_{init(1)}(C\odot B),& Q=(C^TC)\otimes (B^TB)\\
U=X_{init(2)}(C\odot A),& V=(C^TC)\otimes (A^TA)
\end{aligned}</script></li>
<li><strong>更新</strong>: 对每个新进的数据块$X_{new}$<br>a. 对于temporal mode 3, C的更新方式为<script type="math/tex; mode=display">
C=
\left [\begin{aligned}C_{old}  \\C_{new} \\\end{aligned}\right ]
=\left [\begin{aligned}C_{old}&  \\X_{new(3)}((B\odot &A)^T)^\dagger\\\end{aligned}
\right ]</script>b. 对于non-temporal mode 1 and 2, A的更新方式为<script type="math/tex; mode=display">\begin{aligned}P&\gets P+X_{new(1)}(C_{new}\odot B)\\Q&\gets Q+(C^T_{new}C_{new})\otimes (B^TB)\\A&\gets PQ^{-1}\end{aligned}</script>同理B的更新方式为<script type="math/tex; mode=display">\begin{aligned}U&\gets U+X_{new(2)}(C_{new}\odot A)\\V&\gets V+(C^T_{new}C_{new})\otimes (A^TA)\\B&\gets UV^{-1}\end{aligned}</script><h2 id="高阶张量的扩展"><a href="#高阶张量的扩展" class="headerlink" title="高阶张量的扩展"></a>高阶张量的扩展</h2>令$\mathcal{X}_{old}\in \mathbb{R}^{I_1 \times \cdots \times I_{N-1}\times t_{old}}$是$N$阶张量，他的CP分解是$[![A^{(1)}_{old}, \cdots,A^{(N-1)}_{old},A^{(N)}_{old} ]!]$, N-th mode表示时间。一个新的张量$\mathcal{X}_{new}\in \mathbb{R}^{I_1 \times \cdots \times I_{N-1}\times t_{old}}$添加到旧的张量中</li>
</ol>
]]></content>
      <categories>
        <category>张量分解</category>
      </categories>
      <tags>
        <tag>tensor</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>ptx的学习</title>
    <url>/2022/04/14/ptx%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="inline-ptx-assembly-in-CUDA"><a href="#inline-ptx-assembly-in-CUDA" class="headerlink" title="inline ptx assembly in CUDA"></a>inline ptx assembly in CUDA</h1><p>asm()提供了一种将PTX代码插入CUDA的一种方法，形式为<br><figure class="highlight abnf"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;membar.gl;&quot;</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h2><p>asm的基本语法为<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;template-string&quot;</span> : <span class="string">&quot;constraint&quot;</span>(<span class="name">output</span>) : <span class="string">&quot;constraint&quot;</span>(<span class="name">input</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>其中可以有多个’input’输入’output’输出，使用逗号隔开。’template-string’表示用于表明操作的PTX指令，多个ptx指令用分号隔开。</p>
<blockquote>
<p>通常一条指令包括两方面的内容： 操作码和操作数，操作码决定要完成的操作，操作数指参加运算的数据及其所在的单元地址。</p>
</blockquote>
<p>例如<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0, %1, %2;&quot;</span> : <span class="string">&quot;=r&quot;</span>(<span class="name">i</span>):<span class="string">&quot;r&quot;</span>(<span class="name">j</span>),<span class="string">&quot;r&quot;</span>(<span class="name">k</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>template-string中的每个%n表示后面的操作数的顺序，%0是第一个操作数，%1是第二个操作数以此类推。输出操作数总是在输入操作数的前面。这个例子等价于<br><figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">add<span class="selector-class">.s32</span> <span class="selector-tag">i</span>,j,k</span><br></pre></td></tr></table></figure><br>注意，string中的%n的顺序是可变的，上面的指令也可以写成<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0, %2, %1;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">i</span>):<span class="string">&quot;r&quot;</span>(<span class="name">k</span>),<span class="string">&quot;r&quot;</span>(<span class="name">j</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>%n也可以重复<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0,%1,%1;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">i</span>):<span class="string">&quot;r&quot;</span>(<span class="name">k</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>如果这里没有输入操作数，可以舍去最后的冒号<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;mov.s32 %0, 2;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">i</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>如果这里没有输出操作数，可以连接两个冒号<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;mov.s32 r1, %0;&quot;</span>::<span class="string">&quot;r&quot;</span>(<span class="name">i</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>如果想要在PTX中使用%，需要使用两个%来进行转义<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;mov.s32 %0, %%clock;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">x</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>操作数值通过约束指定的任何机制传递。这里的<code>r</code>constraint表示32bit的整型寄存器<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0,%1,%2;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">i</span>):<span class="string">&quot;r&quot;</span>(<span class="name">j</span>),<span class="string">&quot;r&quot;</span>(<span class="name">k</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>会产生下列的代码（通过编译器生成）<br><figure class="highlight avrasm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ld</span>.s32 <span class="built_in">r1</span>, [j]<span class="comment">;</span></span><br><span class="line"><span class="keyword">ld</span>.s32 <span class="built_in">r2</span>, [k]<span class="comment">;</span></span><br><span class="line"><span class="keyword">add</span>.s32 <span class="built_in">r3</span>, <span class="built_in">r1</span>, <span class="built_in">r2</span><span class="comment">;</span></span><br><span class="line"><span class="keyword">st</span>.s32 [i], <span class="built_in">r3</span></span><br></pre></td></tr></table></figure><br>输入操作数在asm语句之前加载到寄存器中，然后将结果寄存器存储到输出操作数中。”=r”中的”=”修饰符指定写入寄存器，还有”+”修饰符modifier指定寄存器是读和写，例如<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0, %0, %1;&quot;</span>:<span class="string">&quot;+r&quot;</span>(<span class="name">i</span>):<span class="string">&quot;r&quot;</span>(<span class="name">j</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>多个指令可以组合成一个asm（）语句;基本上，任何合法的东西都可以放入asm字符串中。通过使用 C/C++ 的隐式string串联，可以将多个指令拆分到多行中。C++样式行末尾注释“//”和经典的C样式注释“/**/”都可以穿插这些字符串用作注释。template-string要在 PTX 中间文件中生成可读输出，最佳做法是以”\n\t”终止每个指令字符串，但最后一个指令字符串除外。</p>
<p>例如，一个例程可以为<br><figure class="highlight wren"><table><tr><td class="code"><pre><span class="line"><span class="variable">__device__</span> int <span class="title function_">cube</span>(<span class="params">int</span> <span class="params">x</span>)</span><br><span class="line">&#123;</span><br><span class="line">    int <span class="variable">y</span>;</span><br><span class="line">    <span class="title function_">asm</span>(<span class="string">&quot;.reg .u32 t1;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span> <span class="comment">// temp reg t1</span></span><br><span class="line">        <span class="string">&quot;mul.lo.u32 t1, %1, %1;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span>    <span class="comment">//t1=x*x</span></span><br><span class="line">        <span class="string">&quot;mul.lo.u32 %0, t1, %1;&quot;</span>        <span class="comment">//y=t1*x</span></span><br><span class="line">        : <span class="string">&quot;=r&quot;</span>(<span class="variable">y</span>):<span class="string">&quot;r&quot;</span>(<span class="variable">x</span>));</span><br><span class="line">    <span class="keyword">return</span> <span class="variable">y</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>如果输出操作数由 asm 指令有条件地更新，则应使用“+”修饰符。在这种情况下，输出操作数是隐式使用的。例如<br><figure class="highlight wren"><table><tr><td class="code"><pre><span class="line"><span class="variable">__device__</span> int <span class="title function_">cond</span>(<span class="params">int</span> <span class="params">x</span>)</span><br><span class="line">&#123;</span><br><span class="line">    int <span class="variable">y</span><span class="operator">=</span><span class="number">0</span>;</span><br><span class="line">    <span class="title function_">asm</span>(<span class="string">&quot;&#123;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span></span><br><span class="line">        <span class="string">&quot;.reg .pred %p;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span></span><br><span class="line">        <span class="string">&quot;setp.eq.s32 %p,%1,34;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span> <span class="comment">// x==34?</span></span><br><span class="line">        <span class="string">&quot;@%p mov.s32 %0, 1;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span>    <span class="comment">// set y to 1 if true</span></span><br><span class="line">        <span class="string">&quot;&#125;&quot;</span>                         <span class="comment">// conceptually y=(x==34)?1:y</span></span><br><span class="line">        :<span class="string">&quot;+r&quot;</span>(<span class="variable">y</span>):<span class="string">&quot;r&quot;</span>(<span class="variable">x</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="variable">y</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
<h2 id="constraint"><a href="#constraint" class="headerlink" title="constraint"></a>constraint</h2><p>每个 PTX 寄存器类型都有一个单独的约束符：<br><figure class="highlight abnf"><table><tr><td class="code"><pre><span class="line"><span class="string">&quot;h&quot;</span><span class="operator">=</span> .u16 reg</span><br><span class="line"><span class="string">&quot;r&quot;</span><span class="operator">=</span> .u32 reg</span><br><span class="line"><span class="string">&quot;l&quot;</span><span class="operator">=</span> .u64 reg</span><br><span class="line"><span class="string">&quot;f&quot;</span><span class="operator">=</span> .f32 reg</span><br><span class="line"><span class="string">&quot;d&quot;</span><span class="operator">=</span> .f64 reg</span><br></pre></td></tr></table></figure><br>约束“n”可用于具有已知值的即时整数操作数。<br>例如<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;cvt.f32.s64 %0,%1;&quot;</span>:<span class="string">&quot;=f&quot;</span>(<span class="name">x</span>):<span class="string">&quot;l&quot;</span>(<span class="name">y</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>将会生成<br><figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">ld.s64</span> rdl,[y]<span class="comment">;</span></span><br><span class="line"><span class="symbol">cvt.f32.s64</span> <span class="built_in">f1</span>,rd1<span class="comment">;</span></span><br><span class="line"><span class="symbol">st.f32</span> [x],<span class="built_in">f1</span><span class="comment">;</span></span><br></pre></td></tr></table></figure></p>
<h2 id="pitfalls"><a href="#pitfalls" class="headerlink" title="pitfalls"></a>pitfalls</h2><p>虽然 asm（） 语句非常灵活和强大，但你可能会遇到一些陷阱</p>
<h3 id="namespace-conflicts"><a href="#namespace-conflicts" class="headerlink" title="namespace conflicts"></a>namespace conflicts</h3><p>如果在程序中多次调用并内联cube function，则会对临时寄存器t1的重复定义。为了避免这个，需要</p>
<ul>
<li>不要内联cube function</li>
<li>在{}中使用’t1’，以便它对每个调用都有单独作用域，即<figure class="highlight wren"><table><tr><td class="code"><pre><span class="line"><span class="variable">__device__</span> int <span class="title function_">cube</span>(<span class="params">int</span> <span class="params">x</span>)</span><br><span class="line">&#123;</span><br><span class="line">    int <span class="variable">y</span>;</span><br><span class="line">    <span class="title function_">asm</span>(<span class="string">&quot;&#123;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span>             <span class="comment">// use braces for scope</span></span><br><span class="line">        <span class="string">&quot;.reg .u32 t1;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span> <span class="comment">// temp reg t1</span></span><br><span class="line">        <span class="string">&quot;mul.lo.u32 t1, %1, %1;<span class="char escape_">\n</span><span class="char escape_">\t</span>&quot;</span>    <span class="comment">//t1=x*x</span></span><br><span class="line">        <span class="string">&quot;mul.lo.u32 %0, t1, %1;&quot;</span>        <span class="comment">//y=t1*x</span></span><br><span class="line">        <span class="string">&quot;&#125;&quot;</span></span><br><span class="line">        : <span class="string">&quot;=r&quot;</span>(<span class="variable">y</span>):<span class="string">&quot;r&quot;</span>(<span class="variable">x</span>));</span><br><span class="line">    <span class="keyword">return</span> <span class="variable">y</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="memory-space-conflicts"><a href="#memory-space-conflicts" class="headerlink" title="memory space conflicts"></a>memory space conflicts</h3><p>由于 asm（） 语句无法知道寄存器所在的内存空间，因此用户必须确保使用适当的 PTX 指令。指向 asm（） 语句的任何指针参数都作为通用地址传递.</p>
<h4 id="incorrect-optimization"><a href="#incorrect-optimization" class="headerlink" title="incorrect optimization"></a>incorrect optimization</h4><p>编译器假定asm语句除了更改输出操作数之外没有任何作用。要确保在生成PTX期间不会删除或移动asm，应使用volatile关键字.<strong>volatile</strong>用于告诉编译器，严禁将此处的汇编语句与其它的语句重组合优化。即：原原本本按原来的样子处理这这里的汇编。<br><figure class="highlight llvm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">asm</span> <span class="keyword">volatile</span>(<span class="string">&quot;mov.u32 %0, %%clock;&quot;</span>::<span class="string">&quot;=r&quot;</span>(<span class="keyword">x</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>通常，写入的任何内存都将被指定为 out 操作数，但如果对用户内存有隐藏的副作用（例如，通过操作数间接访问内存位置），或者如果要停止在生成 PTX 期间围绕 asm（）语句执行的任何内存优化，则可以在第 3 个冒号后添加“memory”clobbers 规范，memory 强制 gcc 编译器假设 RAM 所有内存单元均被汇编指令修改，这样 cpu 中的 registers 和 cache 中已缓存的内存单元中的数据将作废。cpu 将不得不在需要的时候重新读取内存中的数据。这就阻止了 cpu 又将 registers, cache 中的数据用于去优化指令，而避免去访问内存。<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm volatile(<span class="string">&quot;mov.u32 %0, %%clock;&quot;</span>: <span class="string">&quot;=r&quot;</span>(<span class="name">x</span>) :: <span class="string">&quot;memory&quot;</span>)<span class="comment">;</span></span><br><span class="line">asm (<span class="string">&quot;st.u32 [%0], %1;&quot;</span>: <span class="string">&quot;r&quot;</span>(<span class="name">p</span>), <span class="string">&quot;r&quot;</span>(<span class="name">x</span>) :: <span class="string">&quot;memory&quot;</span>)<span class="comment">;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="incorrect-PTX"><a href="#incorrect-PTX" class="headerlink" title="incorrect PTX"></a>incorrect PTX</h3><p>编译器前端不解析asm语句模板字符串，也不知道他的含义甚至不确保ptx是否有效。例如<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;mov.u32 %0,%n1;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">n</span>):<span class="string">&quot;r&quot;</span>(<span class="number">1</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>“%n1”中的“n”修饰符不受支持，它将传递给 ptxas，其中它可能导致未定义的行为。</p>
<h2 id="error-checking"><a href="#error-checking" class="headerlink" title="error checking"></a>error checking</h2><p>以下是编译器将在 inline PTX asm 上执行的一些错误检查</p>
<ul>
<li><p>不允许单个asm操作数只用多个constraint</p>
<figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0,%1,%2;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">i</span>):<span class="string">&quot;rf&quot;</span>(<span class="name">j</span>),<span class="string">&quot;r&quot;</span>(<span class="name">k</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>错误：asm 操作数可能只在<strong>device</strong>/<strong>global</strong>函数中指定一个constraint字母</p>
</li>
<li><p>只允许标量变量作为asm操作数。特别是不允许使用struct类型变量</p>
<figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">int4 i4</span><br><span class="line">asm(<span class="string">&quot;add.s32 %0,%1,%2;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">i4</span>):<span class="string">&quot;r&quot;</span>(<span class="name">j</span>):<span class="string">&quot;r&quot;</span>(<span class="name">k</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>错误：asm操作数必须是标量</p>
</li>
</ul>
<p>PTX中asm constraint所隐含的类型和大小必须与关联操作数的类型和大小匹配。例如其中ci是<code>char</code><br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0,%1,%2;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">ci</span>):<span class="string">&quot;r&quot;</span>(<span class="name">j</span>):<span class="string">&quot;r&quot;</span>(<span class="name">k</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>错误：asm 操作数类型 size（1） 与约束 “r” 所暗示的类型/大小不匹配<br>为了在上面的 asm 语句中使用 “char” 类型变量 “ci”、“cj” 和 “ck”，可以使用类似于以下内容的代码段<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">inttemp = ci<span class="comment">;</span></span><br><span class="line">asm(<span class="string">&quot;add.s32 %0,%1,%2;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">temp</span>):<span class="string">&quot;r&quot;</span>((<span class="name">int</span>)cj),<span class="string">&quot;r&quot;</span>((<span class="name">int</span>)ck))<span class="comment">;</span></span><br><span class="line">ci = temp<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>类型不匹配的另一个示例：对于“float”类型变量“fi”，<br><figure class="highlight lisp"><table><tr><td class="code"><pre><span class="line">asm(<span class="string">&quot;add.s32 %0,%1,%2;&quot;</span>:<span class="string">&quot;=r&quot;</span>(<span class="name">fi</span>):<span class="string">&quot;r&quot;</span>(<span class="name">j</span>),<span class="string">&quot;r&quot;</span>(<span class="name">k</span>))<span class="comment">;</span></span><br></pre></td></tr></table></figure><br>错误：asm 操作数类型 size（4） 与约束 “r” 所隐含的类型/大小不匹配</p>
<h1 id="mma-sp-with-sparse-matrix-A"><a href="#mma-sp-with-sparse-matrix-A" class="headerlink" title="mma.sp with sparse matrix A"></a>mma.sp with sparse matrix A</h1><p>本节主要是用于A100上sparse tensor core的研究。<br>warp-level指令mma.sp,作为mma的变体。<br>当A是结构化稀疏矩阵时，每行有50%的零值以特定的形状分布时，可以使用mma.sp进行spmm操作。<br>对于一个 $M\times N\times K$的mma.sp操作，大小为$M\times K$的矩阵A的所有元素会打包进入一个$M\times K / 2$的矩阵中。在A的每行中，用K/2大小的内存来存储非零元（非零元不满K/2的话会进行填充？？），同样地有K/2大小的空间来存储元素的映射关系（列索引），这个被称为metadata。</p>
<h2 id="sparse-matrix-storage"><a href="#sparse-matrix-storage" class="headerlink" title="sparse matrix storage"></a>sparse matrix storage</h2><p>稀疏矩阵A的粒度定义为矩阵行的子块中非零元素数目与该子块中元素总数的比率，其中子块的大小是特定的。例如，$16\times 16$的矩阵，稀疏度是2:4，即矩阵行中所有的4元素的向量（4个连续元素的子块）包含两个零值。子块中所有非零元的索引被存在metadata中。在一组4个连续的线程中，一个或多个线程根据矩阵形状会存储整个group的metadata，这些线程是使用附加的sparsity selector运算符指定的。<br><img src="https://raw.githubusercontent.com/HURONG0510/blogpic/main/20220614/微信截图_20220614224319.7hua0bi57s00.png" alt=""><br><img src="https://raw.githubusercontent.com/HURONG0510/blogpic/main/20220614/微信截图_20220614225403.9qxwud8sstk.png" alt=""><br>上图表示一个4线程的group负责的sub-chunk，有两个非零元x和y，在mma.sp中元素存在sparse matrix operand，索引存在metadata中（相对列索引）。</p>
<p>不同矩阵形状和数据类型的粒度如下所示</p>
<h3 id="sparse-mma-sp-with-half-precision-and-bf16-type"><a href="#sparse-mma-sp-with-half-precision-and-bf16-type" class="headerlink" title="sparse mma.sp with half-precision and .bf16 type"></a>sparse mma.sp with half-precision and .bf16 type</h3><p>sparsity selector表示哪些线程存储metadata：</p>
<ul>
<li>m16n8k16：group中的一个线程存储metadata，在{0,1,2,3}中选择</li>
<li>m16n8k32: group中的一对线程存储metadata，在{0，1}和{2，3}中各选一个</li>
</ul>
<h3 id="sparse-mma-sp-with-tf32-type"><a href="#sparse-mma-sp-with-tf32-type" class="headerlink" title="sparse mma.sp with .tf32 type"></a>sparse mma.sp with .tf32 type</h3><p>当矩阵A的元素类型是.tf32，A的结构化稀疏的粒度必须是1:2。元素存在operand数组中，索引存在matadata中。需要注意的这里还是用4bit来存储索引,只有0b1110和0b0100是有效的索引值，其余索引会产生未定义行为。</p>
<blockquote>
<p>这里的子块的长度是2，又是1:2的粒度，却不用1bit来存储索引，这里要注意一下。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/HURONG0510/blogpic/main/20220614/微信截图_20220615133612.sopfz9sioio.png" alt=""><br>sparse selector决定哪些线程获得metadata</p>
<ul>
<li>m16n8k16：group中的一个线程存储metadata，在{0,1,2,3}中选择</li>
<li>m16n8k32: group中的一对线程存储metadata，在{0，1}和{2，3}中各选一个<blockquote>
<p>虽然subchunk变成了2，但还是4个线程在协作</p>
</blockquote>
</li>
</ul>
<h3 id="sparse-mma-sp-with-integer-type"><a href="#sparse-mma-sp-with-integer-type" class="headerlink" title="sparse mma.sp with integer type"></a>sparse mma.sp with integer type</h3>]]></content>
      <categories>
        <category>cuda进阶学习</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>ptx</tag>
      </tags>
  </entry>
  <entry>
    <title>warp function</title>
    <url>/2022/04/08/warp-function/</url>
    <content><![CDATA[<p>以下是关于warp function的一些理解<br>主要参考资料是nVidia的官方文档</p>
<h1 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h1><p>warp的概念不加赘述，且建议在<strong>capability 7.x以上且cuda9.0以上</strong>的GPU上测试，有些函数在例如 <strong>any, </strong>all, and __ballot等在cuda9.0上已经被移除。</p>
<blockquote>
<p>predicate表示线程的一种状况<br>以下所有的实例，使用一个block，block块的大小是32，所说的线程号就是laneID<br>mask中的第i个bit表示第i个线程</p>
</blockquote>
<h1 id="warp-vote-functions"><a href="#warp-vote-functions" class="headerlink" title="warp vote functions"></a>warp vote functions</h1><ul>
<li>允许给定warp中的线程执行规约和广播操作</li>
<li>所有线程的lane的mask必须一致</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> __all_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br><span class="line"><span class="type">int</span> __any_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br><span class="line"><span class="type">unsigned</span> __ballot_sync(<span class="type">unsigned</span> mask, <span class="type">int</span> predicate);</span><br><span class="line"><span class="type">unsigned</span> __activemask()</span><br></pre></td></tr></table></figure>
<p><code>__all_sync</code> 评估<code>mask</code>中所有non-exited的线程（线程对应mask的位是1）的predicate，返回非零值当且仅当mask中对应线程的predicate都是非零的。</p>
<p><code>__any_sync</code> 评估<code>mask</code>中所有non-exited的线程（线程对应mask的位是1）的predicate，返回非零值当且仅当mask中对应线程的predicate存在非零的。</p>
<p><code>_ballot_sync</code>评估<code>mask</code>中所有non-exited线程的predicate，返回一个unsigned数，其第N位为1当且仅当<code>mask</code>中线程的predicate非零</p>
<p><code>__activemask()</code> 返回unsigned数 <code>mask</code>，表示这个warp中所有的active状态的线程</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__global__ <span class="type">void</span> <span class="title function_">wall</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="type">int</span> laneId=threadIdx.x &amp; <span class="number">0x1f</span>;</span><br><span class="line">    <span class="type">int</span> predicate= laneId%<span class="number">2</span>;</span><br><span class="line">    <span class="type">unsigned</span> n;</span><br><span class="line">    n=__all_sync(<span class="number">0x55555555</span>,predicate);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Thread %d final n= %x\n&quot;</span>, threadIdx.x, n);</span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>0x55555555</code>表示所有偶数位是1，奇数位是0。<code>predicate</code>是所有的偶数线程是0，奇数线程是1。在<code>__all_sync</code>下，所有线程的n是0，因为其只统计mask中non-exited的线程，<code>0x55555555</code>使得<code>__all_sync</code>只检查偶数位的线程，结果偶数位线程的predicate都是0。其余函数同理。</p>
<h1 id="warp-match-functions"><a href="#warp-match-functions" class="headerlink" title="warp match functions"></a>warp match functions</h1><ul>
<li>执行warp内线程之间变量的广播和比较操作</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_any_sync(<span class="type">unsigned</span> mask, T value);</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> __match_all_sync(<span class="type">unsigned</span>  mask, T value, <span class="type">int</span> *pred)</span><br><span class="line"><span class="comment">//T可以是int/unsigned int/long/unsigned long/long long/unsigned long long/float/double</span></span><br></pre></td></tr></table></figure>
<h1 id="warp-shuffle-function"><a href="#warp-shuffle-function" class="headerlink" title="warp shuffle function"></a>warp shuffle function</h1><ul>
<li>交换warp内部的线程的值</li>
<li>采用可选的width，必须是2的幂次，且不能大于warpsize</li>
<li>mask的值没有影响，不管mask是什么，结果都一致<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">T __shfl_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> srcLane, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_up_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_down_sync(<span class="type">unsigned</span> mask, T var, <span class="type">unsigned</span> <span class="type">int</span> delta, <span class="type">int</span> width=warpSize);</span><br><span class="line">T __shfl_xor_sync(<span class="type">unsigned</span> mask, T var, <span class="type">int</span> laneMask, <span class="type">int</span> width=warpSize);</span><br><span class="line"></span><br><span class="line"><span class="comment">//T can be int, unsigned int, long, unsigned long, long long, unsigned long long, float or double. With the cuda_fp16.h header included, T can also be __halfor __half2. Similarly, with the cuda_bf16.h header included, T can also be __nv_bfloat16 or __nv_bfloat162.</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<code>__shfl_sync()</code>返回srclane号线程的var，<code>width</code>将warpsize的线程数进行划分，每个子段长度为<code>width</code>。每个<code>width</code>中的线程得到<code>srclane</code>所指示的<code>var</code>，注意这里<code>srclane</code>都是<code>width</code>中的相对位置。<br>若是每个线程的<code>var</code>是它本身的线程值，那么<blockquote>
<p><code>__shfl_sync(mask,threadIdx.x,0,4)</code>，得到的结果是 4x0, 4x4, 4x8, 4x12, 4x16, 4x20, 4x24, 4x28, 4x32<br><code>__shfl_sync(mask,x,2,8)</code>, 得到的结果是2x8, 10x8, 18x8, 26x8</p>
</blockquote>
</li>
</ul>
<p><code>_shfl_up_sync()</code> 返回向前偏移为 <code>delta</code> 的线程中的变量 <code>var</code> 的值，其余线程返回0。<code>width</code>将warpsize划分成warpsize/width个部分，每个部分返回的是当前的线程-delta的线程的value，若是减法结果为-，那么结果就不会变。注意这里是相对值，记得上面这个减法必须是要一个分组内。</p>
<blockquote>
<p>例如<code>n=__shfl_up_sync(0xffffffff,value,15,16);</code><br>结果是0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 0 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 16</p>
</blockquote>
<p><code>__shfl_down_sync()</code> 线程返回向后偏移为 delta 的线程中的变量 var 的值，其余线程返回0 。</p>
<blockquote>
<p>调用 <code>__shfl_down_sync(mask, x, 2, 16)</code>; 则标号为 0-13 的线程分别获得标号为 2-15 的线程中变量 x 的值；标号为 16 -29 的线程分别获得标号为 18 - 31 的线程中变量 x 的值。</p>
</blockquote>
<p><code>__shfl_xor_sync()</code>通过对调用者的通道ID与<code>laneMask</code>进行按位异或（XOR）运算来计算源通道ID。返回值为计算所得源通道中的var值。此模式实现了蝶形寻址模式。如果<code>width</code>小于warpsize，那么对于异或的结果，若是处于前面的group，那么可以获取异或的结果，若是处于后面的group，则会返回本身的var</p>
<blockquote>
<p>例如<code>n=__shfl_xor_sync(0,threadIdx.x,3,4);</code>的结果是3 2 1 0 7 6 5 4 11 10 9 8 15 14 13 12 19 18 17 16 23 22 21 27 26 25 24 31 30 29 28<br>例如<code>n=__shfl_xor_sync(0,threadIdx.x,3,2);</code>的结果是0 1 1 0 4 5 5 4 8 9 9 8 12 13 13 12 16 17 17 16 20 21 21 20 24 25 25 24 28 29 29 28</p>
</blockquote>
]]></content>
      <categories>
        <category>cuda基础学习</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title>动态并行</title>
    <url>/2022/02/28/%E5%8A%A8%E6%80%81%E5%B9%B6%E8%A1%8C/</url>
    <content><![CDATA[<h1 id="动态并行"><a href="#动态并行" class="headerlink" title="动态并行"></a>动态并行</h1><p>CUDA的动态并行允许在GPU端直接创建和同步新的GPU内核。在GPU端直接创建工作的能力可以减少在主机和设备之间传输执行控制和数据的需求，因为在设备上执行的线程可以在<strong>运行时</strong>决定启动配置。</p>
<h2 id="嵌套执行"><a href="#嵌套执行" class="headerlink" title="嵌套执行"></a>嵌套执行</h2><p>在动态并行中，内核执行分为两种类型：父母和孩子。父线程、父线程块或父网格启动一个新的网格，即子网格。子线程、子线程块或子网格被父母启动。子网格必须在父线程、父线程块或父网格完成之前完成。只有在所有的子网格都完成之后，父母才会完成。</p>
<p>下图说明了父网格和子网格的使用范围。主机线程配置和启动父网格，父网格配置和启动子网格。在线程创建的所有子网格都完成之后，父网格才会完成。如果调用的线程没有显式同步启动子网格，那么运行时保证父母和孩子之间的隐式同步。下图在父线程中设置了栅栏，从而可以与其子网格显式同步。</p>
<div  align="center">    
 <img src="https://cdn.jsdelivr.net/gh/HURONG0510/blogpic@main/20220228/20220228120642.58dizq8tk7o0.webp" width = "400" height = "200" />
</div>

<blockquote>
<p>设置栅栏，需要补充</p>
</blockquote>
<ul>
<li>设备线程中的网格启动，在线程块间是可见的。在线程块中，只有所有线程创建的子网格完成之后，线程块才结束。如果线程块中的线程在所有网格完成之前退出，那么在那些子网格上隐式同步会被触发。</li>
<li>当父母启动一个子网格，父线程块与孩子显式同步之后，孩子才能开始执行。</li>
<li>父网格和子网格共享相同的全局和常量内存存储，但他们有不同的局部内存和共享内存。</li>
<li>父网格和子网格可以对全局内存并发存取。有两个时刻，子网格和他的父线程见到的内存完全相同：<ul>
<li>子网格开始时</li>
<li>子网格完成时</li>
</ul>
</li>
<li>共享内存和局部内存分别于线程块或线程来说是私有的，同时在父母和孩子之间不是可见或一致的。局部内存对线程来说是私有存储，并且对该线程外部不可见。当启动一个子网格时，向局部内存传递一个指针作为参数是无效的。</li>
</ul>
<p>下图表示在GPU上嵌套hello world，每个网格的第0号线程输出hello world。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><img src="https://raw.githubusercontent.com/HURONG0510/blogpic/main/20220228/微信截图_20220228213943.1z5k28bd1se8.png" alt=""></th>
<th><img src="https://raw.githubusercontent.com/HURONG0510/blogpic/main/20220228/微信截图_20220228214303.20t4lvy2suw0.png" alt=""></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
</tr>
</tbody>
</table>
</div>
<p>因为动态并行是由设备运行时库所支持的，所以函数必须在命令行使用 <strong>-lcudadevrt</strong>  进行明确链接。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">nvcc -<span class="built_in">arch</span>=sm_35 -rdc=<span class="literal">true</span> nestedHelloWorld.cu -o hestHelloWorld -lcudadevrt</span><br></pre></td></tr></table></figure></p>
<p>当<strong>-rdc</strong> 标志为true，他强制生成可重定位的设备代码，这是动态并行的一个要求。</p>
<h2 id="动态并行的限制条件"><a href="#动态并行的限制条件" class="headerlink" title="动态并行的限制条件"></a>动态并行的限制条件</h2><p>动态并行只有在计算能力为3.5或更高的设备上才能被支持。通过动态并行调用的内核不能在物理方面独立的设备上启动。动态并行的最大嵌套深度限制为24，但实际上，在每一个新的级别中大多数内核受限于设备运行时系统需要的内存数量。因为为了对每个嵌套层中的父网格和子网格之间进行同步管理，设备运行时需要保留额外的内存。</p>
]]></content>
      <categories>
        <category>cuda基础学习</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络中的结构化张量</title>
    <url>/2022/05/30/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BB%93%E6%9E%84%E5%8C%96%E5%BC%A0%E9%87%8F/</url>
    <content><![CDATA[<p>CNN要求大量的参数去实现高准确度，造成大量的训练时间、长的inference时间和大量的内存。模型压缩作为一种常见的DNN中减少参数量的方式。例如 parameter quantization, knowledge distillation and network pruning。<br>为了能够从稀疏张量核心中受益，输入矩阵必须采用特定的2：4结构。我们将在本文后面描述2：4结构的特征。我们使用2：4修剪来参考修剪结果在2：4结构中的修剪方法。<br>filter pruning是一个标准的网络剪枝的方法。他评估filter的重要性并移除冗余的数据。因此选择一个合适的评估指标来删除冗余权重。这个想法是生成一个较小的模型，该模型可以通过首先进行filter pruning来加速sparse tensor core。然后，我们应用 2：4 修剪，以确保权重矩阵是 2：4 的结构，以利用sparse tensor core提供的性能。</p>
<h1 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h1><h2 id="sparse-tensor-core"><a href="#sparse-tensor-core" class="headerlink" title="sparse tensor core"></a>sparse tensor core</h2><p>sparse tensor core要求输入矩阵必须采用 2:4 格式，以利用sparse tensor core硬件。<br>我们把矩阵的每行划分成四个元素的组，每个组至少有两个值为0的元素，则这个矩阵是2:4 format。</p>
<h2 id="unstructured-pruning"><a href="#unstructured-pruning" class="headerlink" title="unstructured pruning"></a>unstructured pruning</h2><p>非结构化剪枝移除模型中不重要的权重来减少参数。需要将矩阵编码成特殊格式，以跳过零值来减少内存使用。这些格式是传统的CSR或者CSC。但是由于矩阵的稀疏性，需要特殊的硬件和软件支持，为密集计算的硬件和库在剪枝后无法给予稀疏矩阵提供高效的训练和高速的推理。</p>
<h2 id="structed-pruning"><a href="#structed-pruning" class="headerlink" title="structed pruning"></a>structed pruning</h2><p>结构化剪枝使用结构化的方式来避免非结构化剪枝的问题。</p>
]]></content>
  </entry>
  <entry>
    <title>DNN的入门</title>
    <url>/2022/04/09/%E9%9D%9E%E7%BB%93%E6%9E%84%E7%A8%80%E7%96%8F/</url>
    <content><![CDATA[<h1 id="Deep-Neural-Networks"><a href="#Deep-Neural-Networks" class="headerlink" title="Deep Neural Networks"></a>Deep Neural Networks</h1><p><img src="https://raw.githubusercontent.com/HURONG0510/blogpic/main/20220409/微信截图_20220409221424.7c2prqkmli80.png" alt="神经网络简单举例"></p>
<p>DNN是一类执行分类、拟合和其它数据分析任务的模型。他们是由一系列的layer组成，每个layer在输入向量上执行线性或赋形变化，然后在生成的向量上应用非线性函数，称为激活函数。数据依次遍历这些层得到最终结果。执行其它功能的layer也可能出现在网络中，包含降低数据维度的pooling layer，提高网络对从未见过的数据性能的regularization layer。<br>神经网络中的数据和变化通常表示为tensor。网络在训练期间设置的值称为parameters或weights，将向网络呈现数据和更新参数的单一步骤称为一个training iteration。对所有训练数据集执行示例过程，直到网络得到充分训练，每次对整个训练数据集进行网络训练时，都被称为一个epoch。一个神经网络通常被训练上十或上百次。在训练过程中控制网络训练或操作但未被修改的值被称为hyperparameters.一旦网络被训练完成，参数就固定，模型可以用来分析新数据，这个过程称为inference。</p>
<p>最简单的神经网络layer是dense或fully-connected layer。密集层输入长度为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></svg></mjx-container>的向量<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>,产生长度为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.378ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 1051 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g></g></g></svg></mjx-container>的向量<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 490 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>，其参数由权重矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="11.389ex" height="2.004ex" role="img" focusable="false" viewBox="0 -846 5033.8 886"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1325.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2270.6,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(792,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mo" transform="translate(1051,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1829,0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></g></g></svg></mjx-container>，他的实现函数为<script type="math/tex">y=f(Wx+b)</script>其中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="3.005ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1328 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mo" transform="translate(939,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>是activation function。深度神经网络在输入和输出之间具有许多层，使它们能够学习非常复杂的函数，并减少或消除对特征提取的需求。与其他统计模型相比，DNN 具有非常多的参数，通常从数十万到数亿个参数不等。</p>
<p>现代DNN在根据其训练数据进行评估时通常可以达到非常高的准确性，但在未知数据集上获得类似性能可能会很大难度。已经开发了很多方法来改善未知数据集的模型性能，有时以降低训练数据的性能为代价，这些方法称为regularization。它们包括对训练过程的修改、对优化器目标的修改以及神经网络中的其他层，例如 dropout和batch normalization层。 </p>
<p>接下来是详细描述，来源是<a href="https://www.cnblogs.com/pinard/category/894694.html">刘建平的博客</a>.</p>
<h2 id="感知机到神经网络"><a href="#感知机到神经网络" class="headerlink" title="感知机到神经网络"></a>感知机到神经网络</h2><p>感知机的模型如下，他是由若干输入和一个输出组成<br><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220110637351-839081092.png" alt=""><br>输入和输出学习到一个线性关系，得到中间的输出结果</p>
<script type="math/tex; mode=display">z=\sum\limits_{i=1}^mw_ix_i + b</script><p>接着是一个神经元激活函数</p>
<script type="math/tex; mode=display">sign(z)= \begin{cases} -1& {z<0}\\ 1& {z\geq 0} \end{cases}</script><p>神经网络在感知机的模型上做了扩展，总结如下有三点</p>
<ol>
<li>加入了隐藏层<br><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220111519210-2096738104.png" alt=""></li>
<li>输出层的神经元有多个<br><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122136538-2002639053.png" alt=""></li>
<li>激活函数的扩展</li>
</ol>
<h2 id="DNN的基本结构"><a href="#DNN的基本结构" class="headerlink" title="DNN的基本结构"></a>DNN的基本结构</h2><p>DNN可以理解为有很多隐藏层的神经网络，DNN有时也被称为multi-layer perception, MLP.<br>从DNN按不同层的位置划分，DNN内部的神经网络层可以分为三类，输入层，隐藏层和输出层。第一层是输入层，最后一层是输出层，而中间的层数是隐藏层。<br><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220122323148-1704308672.png" alt=""><br>层与层之间是全连接的，也就是说第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。<br>由于DNN的层数很多，因此参数有很多。</p>
<ul>
<li>权重参数<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>。第l-1层的第k个神经元到第l层的第j个神经元的权重参数定义为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.04ex" xmlns="http://www.w3.org/2000/svg" width="3.3ex" height="2.972ex" role="img" focusable="false" viewBox="0 -853.7 1458.7 1313.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(749,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="TeXAtom" transform="translate(749,-315.5) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mi" transform="translate(412,0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></g></svg></mjx-container>.注意，输入层没有<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>参数。以下图三层的DNN为例，第二层的第4个神经元到第三层的第2个神经元的权重参数定义为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.69ex" xmlns="http://www.w3.org/2000/svg" width="3.407ex" height="2.589ex" role="img" focusable="false" viewBox="0 -839.4 1506.1 1144.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mn" transform="translate(749,369.2) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="TeXAtom" transform="translate(749,-305.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(500,0)"></path></g></g></g></g></g></svg></mjx-container><br><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220134346179-1092973493.png" alt=""></li>
<li>偏倚<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>的定义。第二层的第三个神经元的偏倚定义为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.685ex" xmlns="http://www.w3.org/2000/svg" width="1.958ex" height="2.572ex" role="img" focusable="false" viewBox="0 -833.9 865.6 1136.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(462,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mn" transform="translate(462,-287.2) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></g></svg></mjx-container></li>
</ul>
<h2 id="DNN前向传播算法数学原理"><a href="#DNN前向传播算法数学原理" class="headerlink" title="DNN前向传播算法数学原理"></a>DNN前向传播算法数学原理</h2><p>假设我们选择的激活函数是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.104ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1814 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(571,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(960,0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(1425,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，隐藏层和输出层的输出值为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container>，则对于下图的三层DNN，利用和感知机一样的思路，我们可以使用上一层的输出计算下一层的输出，就是DNN前向传播算法。<br><img src="https://images2015.cnblogs.com/blog/1042406/201702/1042406-20170220142015116-1152957081.png" alt=""><br>对于第二层的输出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.685ex" xmlns="http://www.w3.org/2000/svg" width="8.566ex" height="2.572ex" role="img" focusable="false" viewBox="0 -833.9 3786 1136.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mn" transform="translate(562,-287.9) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(965.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(1410.2,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mn" transform="translate(562,-287.9) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2375.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(2820.4,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mn" transform="translate(562,-287.2) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></g></svg></mjx-container>，我们有</p>
<script type="math/tex; mode=display">a_1^2=\sigma(z_1^2) = \sigma(w_{11}^2x_1 + w_{12}^2x_2 + w_{13}^2x_3 + b_1^{2})\\
a_2^2=\sigma(z_2^2) = \sigma(w_{21}^2x_1 + w_{22}^2x_2 + w_{23}^2x_3 + b_2^{2})\\
a_3^2=\sigma(z_3^2) = \sigma(w_{31}^2x_1 + w_{32}^2x_2 + w_{33}^2x_3 + b_3^{2})</script><p>对于第三层的输出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.673ex" xmlns="http://www.w3.org/2000/svg" width="2.185ex" height="2.572ex" role="img" focusable="false" viewBox="0 -839.4 965.6 1136.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,369.2) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mn" transform="translate(562,-297.3) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container>，有</p>
<script type="math/tex; mode=display">a_3^1 =\sigma(z_3^1) = \sigma(w_{11}^3a_1^2 + w_{12}^3a_2^2 + w_{13}^3a_3^2 + b_1^{3})</script><p>假设第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="4.571ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2020.4 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(520.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1520.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>层共有m个神经元，则对于第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>个神经元的输出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.987ex" xmlns="http://www.w3.org/2000/svg" width="2.044ex" height="2.919ex" role="img" focusable="false" viewBox="0 -853.7 903.3 1290.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(562,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><g data-mml-node="mi" transform="translate(562,-292.2) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>,我们有</p>
<script type="math/tex; mode=display">a_j^l = \sigma(z_j^l) = \sigma(\sum\limits_{k=1}^mw_{jk}^la_k^{l-1} + b_j^l)</script><p>使用矩阵去描述的话就是会简洁很多。假设第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="4.571ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2020.4 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(520.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1520.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>层共有<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 878 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>个神经元，而第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>层共有<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.357ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 600 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>个神经元，则第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>层的权重系数<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.62ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 716 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g></svg></mjx-container>组成看一个<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="6.11ex" height="1.136ex" role="img" focusable="false" viewBox="0 -491 2700.4 502"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1822.4,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>的矩阵<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="3.16ex" height="1.981ex" role="img" focusable="false" viewBox="0 -853.7 1396.9 875.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></svg></mjx-container>，第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>层的偏倚<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>组成了一个<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.532ex" role="img" focusable="false" viewBox="0 -666 2322.4 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>的向量<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.635ex" height="1.956ex" role="img" focusable="false" viewBox="0 -853.7 722.7 864.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(462,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></svg></mjx-container>，第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="4.571ex" height="1.756ex" role="img" focusable="false" viewBox="0 -694 2020.4 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(520.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1520.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>层的输出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container>组成了一个<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.883ex" height="1.532ex" role="img" focusable="false" viewBox="0 -666 2600.4 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1100.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(2100.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>的向量，第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>层的未激活前的线性输出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container>组成了一个<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.532ex" role="img" focusable="false" viewBox="0 -666 2322.4 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>的向量<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.717ex" height="1.956ex" role="img" focusable="false" viewBox="0 -853.7 758.7 864.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></svg></mjx-container>，第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.674ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 298 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></svg></mjx-container>层的输出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewBox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g></g></g></svg></mjx-container>组成了一个<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="5.254ex" height="1.532ex" role="img" focusable="false" viewBox="0 -666 2322.4 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(822.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(1822.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>的向量<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.861ex" height="1.954ex" role="img" focusable="false" viewBox="0 -853.7 822.7 863.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></svg></mjx-container>.用矩阵表示，第l层的输出为</p>
<script type="math/tex; mode=display">a^l = \sigma(z^l) = \sigma(W^la^{l-1} + b^l)</script><h2 id="DNN反向传播算法的基本思路"><a href="#DNN反向传播算法的基本思路" class="headerlink" title="DNN反向传播算法的基本思路"></a>DNN反向传播算法的基本思路</h2><p>回到我们监督学习的一般问题，假设我们有 m 个训练样本：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="30.951ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 13680.6 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="mo" transform="translate(500,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(889,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1897.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2342.2,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(3268.8,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3657.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(4102.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4491.4,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(605,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(5500,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5944.7,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(523,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(6871.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7260.2,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(7704.9,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(9043.5,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(9488.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(9877.2,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(605,-150) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(11153.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(11597.7,0)"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(523,-150) scale(0.707)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(12791.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(13180.6,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container>, 其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>为输入向量，特征维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.455ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1527 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mi" transform="translate(927,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>, 而 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 490 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>为输出向量，特征维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="4.432ex" height="1.773ex" role="img" focusable="false" viewBox="0 -626 1958.9 783.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g><g data-mml-node="mi" transform="translate(1025.9,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1597.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></svg></mjx-container>。我们需要利用这 m 个样本训练出一个模型，当有一个新的测试样本 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.967ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3521.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389,0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(605,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(361,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(827,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1296,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2215.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(2660.3,0)"><path data-c="3F" d="M226 668Q190 668 162 656T124 632L114 621Q116 621 119 620T130 616T145 607T157 591T162 567Q162 544 147 529T109 514T71 528T55 566Q55 625 100 661T199 704Q201 704 210 704T224 705H228Q281 705 320 692T378 656T407 612T416 567Q416 503 361 462Q267 395 247 303Q242 279 242 241V224Q242 205 239 202T222 198T205 201T202 218V249Q204 320 220 371T255 445T292 491T315 537Q317 546 317 574V587Q317 604 315 615T304 640T277 661T226 668ZM162 61Q162 89 180 105T224 121Q247 119 264 104T281 61Q281 31 264 16T222 1Q197 1 180 16T162 61Z"></path></g><g data-mml-node="mo" transform="translate(3132.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>来到时, 我们可以预测 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="3.947ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 1744.7 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(523,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(361,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(827,0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(1296,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></g></g></svg></mjx-container>向量的输出。 </p>
<p>如果我们采用 DNN 的模型，即我们使输入层有 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.455ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1527 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mi" transform="translate(927,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>个神经元，而输出层有 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="4.432ex" height="1.773ex" role="img" focusable="false" viewBox="0 -626 1958.9 783.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g><g data-mml-node="mi" transform="translate(1025.9,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1597.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></svg></mjx-container>个神经元。再加上一些含有若干神经元的隐藏层。此时我们需要找到合适的所有隐藏层和输出层对应的线性系数矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>, 偏倚向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>, 让所有的训练样本输入计算出的输出尽可能的等于或很接近样本输出。<br>可以用一个合适的损失函数来度量训练样本的输出损失，接着对这个损失函数进行优化求最小化的极值，对应的一系列线性系数矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="2.371ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 1048 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>, 偏倚向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.971ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 429 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>即为我们的最终结果。<br>对 DNN 的损失函数用梯度下降法进行迭代优化求极小值的过程即为我们的反向传播算法。</p>
<p>在进行 DNN 反向传播算法前，我们需要选择一个损失函数，来度量训练样本计算出的输出和真实的训练样本输出之间的损失。你也许会问：训练样本计算出的输出是怎么得来的？这 个输出是随机选择一系列 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="4.348ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 1921.7 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1048,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1492.7,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>, 用我们上一节的前向传播算法计算出来的。即通过一系列的计算：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="27.183ex" height="2.497ex" role="img" focusable="false" viewBox="0 -853.7 12015 1103.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><g data-mml-node="mo" transform="translate(1100.5,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2156.3,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(2727.3,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(3116.3,0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(498,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><g data-mml-node="mo" transform="translate(3875,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4541.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(5597.5,0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(6168.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(6557.5,0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1136.2,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><g data-mml-node="msup" transform="translate(7954.5,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(562,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(298,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1076,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(9903.1,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msup" transform="translate(10903.3,0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(462,363) scale(0.707)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><g data-mml-node="mo" transform="translate(11626,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>。计算到输出层第 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container>层对应的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="2.474ex" height="1.937ex" role="img" focusable="false" viewBox="0 -846 1093.5 856"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,363) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></g></svg></mjx-container>即为前向传播算法计算出来的输出。</p>
<p>回到损失函数，DNN 可选择的损失函数有不少，为了专注算法，这里我们使用最常见的均方差来度量损失。即对于每个样本，我们期望最小化下式：</p>
<script type="math/tex; mode=display">J(W,b,x,y) = \frac{1}{2}||a^L-y||_2^2</script><p>　　　　其中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="2.474ex" height="1.937ex" role="img" focusable="false" viewBox="0 -846 1093.5 856"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,363) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></g></svg></mjx-container>和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.109ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 490 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>为特征维度为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="4.432ex" height="1.773ex" role="img" focusable="false" viewBox="0 -626 1958.9 783.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(633,-150) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g><g data-mml-node="mi" transform="translate(1025.9,0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1597.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g></g></svg></mjx-container>的向量, 而 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.564ex" xmlns="http://www.w3.org/2000/svg" width="4.963ex" height="2.26ex" role="img" focusable="false" viewBox="0 -749.5 2193.6 999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(278,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(556,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(1201,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(1479,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(311,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>为 S 的 L2 范数。</p>
<p>损失函数有了，现在我们开始用梯度下降法迭代求解每一层的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="4.348ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 1921.7 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1048,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1492.7,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>。首先是输出层第 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.541ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 681 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g></g></svg></mjx-container>层。注意到输出层的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="4.348ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 1921.7 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1048,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1492.7,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></svg></mjx-container>满足下式：</p>
<script type="math/tex; mode=display">a^L = \sigma(z^L) = \sigma(W^La^{L-1} + b^L)</script><p>这样对于输出层的参数，我们的损失函数变为：</p>
<script type="math/tex; mode=display">J(W,b,x,y) = \frac{1}{2}||a^L-y||_2^2 =  \frac{1}{2}|| \sigma(W^La^{L-1} + b^L)-y||_2^2</script><h1 id="Sparsity"><a href="#Sparsity" class="headerlink" title="Sparsity"></a>Sparsity</h1><p>在DNN中</p>
<h1 id="Neural-Network-Pruning"><a href="#Neural-Network-Pruning" class="headerlink" title="Neural Network Pruning"></a>Neural Network Pruning</h1><p>DNN中的大量参数使他们在实践中难以应用。neural network pruning属于最后一类。修剪方法是用于训练神经网络或调整已经训练过的网络的技术，以便其大量参数变为零。这样做的方式是尽量减少修剪导致的推理准确性的降低。如果我们将每个参数视为表示线性变换中输入和输出之间的连接，则可以将修剪视为删除这些连接。如下所示，pruning fraction或pruning rate是图层或网络中被修剪的connected fraction。<br><img src="https://raw.githubusercontent.com/HURONG0510/blogpic/main/20220409/微信截图_20220409234507.1phl2u16wtuo.png" alt=""><br>使用修剪来加速推理时，一个重要的问题是，所讨论的硬件是否可以实际利用修剪的表示形式。通常需要强加某种结构来管理可以修剪哪些连接，以便看到显着的改进。例如，这可能意味着删除权重矩阵的整行，而不是矩阵的任意元素。以需要遵循特定结构的方式修剪称为结构化修剪，而没有结构要求的修剪称为非结构化修剪。 使用结构化而不是非结构化修剪会带来权衡。粗略地说，结构越严格，可以删除的连接就越少，而不会严重损害网络的准确性。这是因为非结构化修剪可以更自由地删除那些对网络性能损害最小的权重。在实践中，决定是否使用结构化修剪以及如果是，要施加什么结构要求取决于特定应用程序的要求以及用于推理的硬件。</p>
<h1 id="GPU设计准则"><a href="#GPU设计准则" class="headerlink" title="GPU设计准则"></a>GPU设计准则</h1><p>参考了文章</p>
<h2 id="latency-hiding-with-TLP-and-ILP"><a href="#latency-hiding-with-TLP-and-ILP" class="headerlink" title="latency hiding with TLP and ILP"></a>latency hiding with TLP and ILP</h2><p>如果 GPU 上驻留了足够的warp（如果我们有足够的 TLP），则在warp之间切换可以完全隐藏长延时操作的成本。我们将程序中的 TLP 数量量化为占用率occupancy，即可用（发出的）warp与 GPU 可支持的最大warp数之比。更高的占用率产生更好的延迟隐藏能力，这使我们能够实现充分利用。<br>另一种延迟隐藏策略是利用指令级并行性（ILP）及其利用单个线程中多个内存操作的延迟重叠的能力。由于 GPU 的内存系统是深度流水线的，因此线程在变为非活动状态之前可能会发出多个独立的长延迟操作，并且这些多个操作将共同产生与单个操作大致相同的延迟。虽然这产生了显著的性能优势，但它依赖于程序员向硬件公开独立的内存操作。我们可以通过将多个独立任务分配给同一线程（“线程粗化”）来实现此目标。</p>
<p>GPU 具有固定数量的寄存器。TLP 需要许多驻留warp，每个warp都需要寄存器。ILP 会增加每个线程的工作，因此每个线程需要更多的寄存器。因此，TLP和ILP是对立的，要充分利用这两种技术，就需要仔细平衡这两种技术。虽然TLP通常用于所有GPU计算，但ILP是一个较少探索的领域。</p>
<h2 id="load-balancing"><a href="#load-balancing" class="headerlink" title="load-balancing"></a>load-balancing</h2><p>我们现在转向的问题是，确保所有计算单元在每个周期上都做有用的工作，并且从这些warp访问的内存被合并以确保峰值内存性能。在SpMV和SpMM的content中，这种“负载平衡”问题有两个方面：</p>
<ol>
<li>warp之间的负载不均。某些 CTA(block) 或 warp 分配的工作量可能比其他 CTA 或 warp 少，这可能导致这些负载较少的计算单元处于空闲状态，而负载较多的计算单元继续执行有用的工作。在本文中，我们称之为“Type 1”负载不平衡。 </li>
<li>warp内部的负载不均。在两个方面，我们统称为“Type 2”负载不平衡。<br>(a) 某些warp可能没有足够的work来占用warp中的所有32个线程。在这种情况下，线程单元处于空闲状态，我们会损失性能。<br>(b) 某些warp可能会将不同的任务分配给不同的线程。 在这种情况下，线程内的 SIMT 执行意味着某些线程处于空闲状态，而其他线程正在运行;此外，执行过程中的warp分化意味着整个warp中的内存访问不太可能合并。</li>
</ol>
]]></content>
      <categories>
        <category>矩阵计算</category>
      </categories>
      <tags>
        <tag>gpu</tag>
        <tag>matrix</tag>
      </tags>
  </entry>
</search>
